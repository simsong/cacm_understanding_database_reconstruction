\documentclass[jou,apacite]{apa6}
\usepackage{amsmath}

\title{Analysis of a Database Reconstruction Attack on Public Data}
\shorttitle{}

\twoauthors{Christian Martindale}{Simson Garfinkel}
\twoaffiliations{Center for Disclosure Avoidance, U.S. Census Bureau}{Center for Disclosure Avoidance, U.S. Census Bureau}

\abstract{In recent years, a certain type of malicious database attack,
the database reconstruction attack, has become increasingly
more feasible due to rapid advances in attack algorithm
sophistication. Here, we will discuss how these attacks
function, demonstrate their effectiveness and efficiency,
and provide solutions for defending against such an attack.

This paper will answer the following questions:
\begin{enumerate}
    \item What is a database reconstruction attack (DRA)?
    \item To what extent are modern DRAs effective in reconstructing the ground truth data?
    \item How scalable are DRAs with database size?
    \item How can a statistical agency guard public data against a DRA without significantly impacting the accuracy of the data?
\end{enumerate}

We will also develop a vocabulary to facilitate the discussion
of database privacy and formally define some important terms.

\textbf{Keywords: Database Reconstruction Attack, SAT solver, privacy,
disclosure avoidance}}


\begin{document}
\maketitle

\section{Problem Background}
Malicious people often seek to identify individuals from publicly
available data products such as tables and graphs.
Defending against such privacy breaches is
a high priority for statistical agencies, and so researchers
have developed a variety of techniques to prevent database users
from identifying any Personally Identifiable Information (PII)

These techniques include:
\begin{enumerate}
  \item Cell Suppression, where the values of cells with small counts or few possible
        generating combinations are removed from the published table
  \item Row Swapping, where the data rows corresponding to individuals
        with similar values in certain key cells are switched
  \item Bucketing, where numerical values are grouped into
        buckets corresponding to ranges, instead of giving the exact
        values for each entry in the table
  \item Topcoding, where the buckets at the high and low ends
        of the table are given without upper or lower bound (e.g.
        reporting the highest bucket for age as "80+" instead of
        "80-90" and "90-100")
\end{enumerate}

The goal of a Database Reconstruction Attack is to
use public data to create a mathematical system of equations,
which then can be used to reconstruct the original (before the disclosure
avoidance techniques were applied) data set.
While the above techniques are not without merit, this paper will
demonstrate that they alone are insufficient in guarding data against
a modern DRA.

\section{Vocabulary}

\begin{itemize}
\item Database Reconstruction Attack (DRA) - An attempt to determine survey response information from a publicly available data set which has had disclosure avoidance techniques applied to it.

\item Personally Identifiable Information (PII) - Data that can be used to identify a single person.

\item Constraint Equation - A mathematical equation that represents information known to be true for a set of data.

\item Solution Universe (U) - The set of all combinations of a
group of variables that satisfies a certain set of constraint equations.

\item SAT Solver - A program that uses a complex set of heuristics to find the solution universe given a set of constraint equations.

\item Noise Addition - The addition of small random numbers to statistical table values before publication for the purpose of protecting respondent privacy.
\end{itemize}

\section{The Database Reconstruction Attack: An Example}
To understand how a DRA works, let us consider a simple example data table generated from the following survey given to two households:
\begin{verbatim}
Please provide the following for each member
of your family:
Age, Sex, Race, Generation.
\end{verbatim}
Tables 1 and 2 summarize the survey responses for the two households surveyed.
\begin{table}[!htb]
\caption{Household 1}\label{tab1}
\begin{tabular}{cccc}
\hline\\[-1.5ex]
Age & Sex & Race & Gen \\[0.5ex]
\hline\\[-1.5ex]
55 & Female & White & Grandparent\\[0.5ex]
24 & Male & White & Parent\\[0.5ex]
28 & Female & Black & Parent\\[0.5ex]
5 & Female & Black & Child\\[0.5ex]
8 & Male & White & Child\\[0.5ex]
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{Household 2}\label{tab2}
\begin{tabular}{cccc}
\hline\\[-1.5ex]
Age & Sex & Race & Gen \\[0.5ex]
\hline\\[-1.5ex]
50 & Male & White & Parent\\[0.5ex]
45 & Male & White & Parent\\[0.5ex]
15 & Female & White & Child\\[0.5ex]
17 & Female & White & Child\\[0.5ex]
\hline
\end{tabular}
\end{table}

The statistical agency then publishes the following data based
on the survey results: \footnote {Note that statistics formed from groups made up of only one individual are redacted with "XXXX". This is an example of cell suppression used to protect the privacy of those individuals.}

\begin{table}[!htb]
\caption{Survey Results}\label{tab3}
\begin{tabular}{ccc}
\hline\\[-1.5ex]
Group & Number & Average Age \\[0.5ex]
\hline\\[-1.5ex]
Individuals & 9 & 27.44 \\[0.5ex]
Males & 4 & 31.75 \\[0.5ex]
Females & 5 & 24 \\[0.5ex]
Parents & 4 & 36.75 \\[0.5ex]
Grandparents & 1 & XXXX \\[0.5ex]
Children (0-12) & 2 & 6.5 \\[0.5ex]
Children (13-17) & 2 & 16 \\[0.5ex]
Whites & 7 & 30.57 \\[0.5ex]
Blacks & 2 & 24 \\[0.5ex]
Black parents & 1 & XXXX \\[0.5ex]
Black children & 1 & XXXX\\[0.5ex]
White parents & 3 & 39.66 \\[0.5ex]
White children & 3 & 13.33 \\[0.5ex]
Households & 2 & 27.85 \\[0.5ex]
\hline
\end{tabular}
\end{table}

Retabulating these results in a numerical format with the keys in Table 4 gives the results in Table 5.

\begin{table}[!htb]
\caption{Tabulation Key}\label{tab4}
\begin{tabular}{c|c}
\hline\\[-1.5ex]
Key & Value \\[0.5ex]
\hline\\[-1.5ex]
Male & 0 \\[0.5ex]
Female & 1 \\[0.5ex]
White & 0 \\[0.5ex]
Black & 1 \\[0.5ex]
Child & 0 \\[0.5ex]
Parent & 1 \\[0.5ex]
Grandparent & 2 \\[0.5ex]
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{Survey Results}\label{tab5}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & 1 & 24 & 0 & 0 & 1  \\[0.5ex]
2 & 1 & 28 & 1 & 1 & 1  \\[0.5ex]
3 & 1 & 55 & 1 & 0 & 2  \\[0.5ex]
4 & 1 & 5 & 1 & 1 & 0  \\[0.5ex]
5 & 1 & 8 & 0 & 0 & 0  \\[0.5ex]
6 & 2 & 50 & 0 & 0 & 1  \\[0.5ex]
7 & 2 & 45 & 1 & 0 & 0  \\[0.5ex]
8 & 2 & 15 & 1 & 0 & 0  \\[0.5ex]
9 & 2 & 17 & 1 & 0 & 0 \\[0.5ex]
\hline
\end{tabular}
\end{table}


The data in Table 5 is the 'ground truth' database that the attacker wishes to reconstruct. At the start of the attack, the attacker writes the following table of 45 unknowns:

\begin{table}[!htb]
\caption{DRA Unknowns}\label{tab6}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & H1 & A1 & S1 & R1 & G1  \\[0.5ex]
2 & H2 & A2 & S2 & R2 & G2  \\[0.5ex]
3 & H3 & A3 & S3 & R3 & G3  \\[0.5ex]
4 & H4 & A4 & S4 & R4 & G4  \\[0.5ex]
5 & H5 & A5 & S5 & R5 & G5  \\[0.5ex]
6 & H6 & A6 & S6 & R6 & G6  \\[0.5ex]
7 & H7 & A7 & S7 & R7 & G7  \\[0.5ex]
8 & H8 & A8 & S8 & R8 & G8  \\[0.5ex]
9 & H9 & A9 & S9 & R9 & G9  \\[0.5ex]
\hline
\end{tabular}
\end{table}

The reconstruction attack works by identifying constraint
equations given by the data table. For example, we have the
following statistic:
\begin{verbatim}
Individuals: 9, 27.44
\end{verbatim}

This can be written as a linear constraint equation: \footnote{In the actual calculation, mathematical transformations must be applied to avoid floating point rounding errors, but that is beyond the scope of this paper.}
\[(A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + A9)*100/9 = 27.44\]

This equation is in 9 unknowns, and so is unsolvable alone.
However, the attacker can write more constraint equations.

\begin{verbatim}
Grandparents: 1, XXX
\end{verbatim}


Becomes: \footnote{X==Y evaluates to 0 if X!=Y and 1 if $X=Y$.}
\begin{align*}
& (G1==2) + (G2==2) + (G3==2) + (G4==2) + \\
& (G5==2)+ (G6==2) + (G7==2) +\\
& (G8==2) + (G9==2) = 1
\end{align*}

\begin{verbatim}
Children 0-12: 2, 6.5
\end{verbatim}

Becomes the following two equations:

\begin{align*}
& (G1==0) + (G2==0) + (G3==0)+\\
& (G4==0)+  (G5==0) + (G6==0) +\\
& (G7==0) + (G8==0) + G9==0) = 2
\end{align*}
\begin{align*}
& (A1 * (G1==0) + A2 * (G2==0) + A3 * (G3==0) +\\
& A4 * (G4==0) +  A5 * (G5==0) + A6 * (G6==0) + \\
& A7 * (G7==0) + A8 * (G8==0) + A9 * (G9==0)) = 13
\end{align*}
Once the attacker has converted all the published statistics
into equations like the ones above, he will have a system
of a large number of equations in 45 unknowns. This system has one 'true' solution, $S_g$, equivalent to the
ground truth, and possibly many other 'false' solutions, $[S_1...S_n]$.
The Solution Universe, U, is potentially originally quite large, with $U = [S_1...S_n]$

However, each time the agency publishes a new statistic, the attacker
can generate new constraints and therefore narrow
down the set of possible solutions. Eventually, the attacker
will obtain $U = [S_g]$, at which point he has
found the ground truth and reconstructed the database
successfully. Note here that the cell suppression disclosure avoidance technique does not prevent the attacker from performing the reconstruction attack, but rather simply gives him one fewer constraint to work with per cell suppressed.
Even if the number of constraints is insufficient to narrow down U to just one element, the attacker can still often identify personal data because of the high probability that all remaining solutions $S_n$ in $U$ share values for a set of people. For example, if
$U = [S_1, S_2]$ and $S_1$ and $S_2$ both have values $[h_1, a_1, s_1, r_1, g_1]$ in common
(represented mathematically by $S_1 \cap S_2 = [H_1=h_1, A_1=a_1, S_1=s_1, R_1=r_1, G_1=g_1]$),
then we know that $[h_1, a_1, s_1, r_1, g_1]$ must be an actual person in the ground truth set. Thus, the database has failed to protect user privacy even though the attack did not fully reconstruct the database.

\section{Methods of Attack}

The three most common and effective ways of solving a
system of constraint equations are:
\begin{enumerate}

\item \textbf{Brute Force} - trying every possible combination of solutions.\\
\item \textbf{Optimization} - creating a single cost function from the constraints, and then solve the cost function using optimization software. \\
\item \textbf{Using a SAT solver}, which are complex programs that solve Boolean algebra problems.

\end{enumerate}

For large data sets such as the U.S. Census, performing a brute-force attack is infeasible due to the fact that the runtime of brute force programs scales exponentially with the number of unknowns. However, optimizers and SAT solvers are both quite effective because they can reduce the runtime of the program to acceptable (linearly scaling) levels.
Recent advances in SAT solver heuristics have enabled these programs to solve systems with even millions of variables in linear time.
Although public Census data tables are drawn from hundreds of millions of American individuals, the Census publishes billions of statistics from those individual responses, so there are still sufficient constraints to narrow down the solution universe enormously. In later examples,
we will use a SAT solver to demonstrate how quickly these very complex programs can solve large systems of equations.

\section{Revisiting the Example}

In order to show how a SAT solver can be used to rapidly narrow down the solution universe for a data set, we will perform a mock DRA on a new set of responses to the survey given earlier.

The ground truth responses from two new households are given below.
\begin{table}[!htb]
\caption{Survey Results}\label{tab7}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & 1 & 80 & 1 & 1 & 2  \\[0.5ex]
2 & 1 & 40 & 0 & 0 & 1  \\[0.5ex]
3 & 2 & 70 & 1 & 0 & 2  \\[0.5ex]
4 & 2 & 30 & 1 & 1 & 1  \\[0.5ex]
5 & 2 & 90 & 0 & 0 & 2  \\[0.5ex]
6 & 3 & 10 & 0 & 1 & 0  \\[0.5ex]
7 & 3 & 10 & 0 & 1 & 0  \\[0.5ex]
8 & 3 & 10 & 1 & 0 & 0  \\[0.5ex]
9 & 4 & 40 & 1 & 0 & 1 \\[0.5ex]
10 & 4 & 20 & 0 & 1 & 1 \\[0.5ex]
\hline
\end{tabular}
\end{table}

The statistical agency again publishes the following statistics after processing the ground truth responses:

\begin{table}[!htb]
\caption{Data Publication}\label{tab8}
\begin{tabular}{ccc}
\hline\\[-1.5ex]
Group & Number & Average Age \\[0.5ex]
\hline\\[-1.5ex]
Individuals & 10 & 40 \\[0.5ex]
Males & 5 & 34 \\[0.5ex]
Females & 5 & 46 \\[0.5ex]
Parents & 4 & 32.5 \\[0.5ex]
Grandparents & 3 & 80 \\[0.5ex]
Children (total) & 3 & 10 \\[0.5ex]
Children (0-12) & XXXX & XXXX \\[0.5ex]
Children (13-17) & XXXX & XXXX \\[0.5ex]
Whites & 5 & 50 \\[0.5ex]
Blacks & 5 & 30 \\[0.5ex]
Black parents & 2 & AAAA \\[0.5ex]
Black children & 2 & AAAA\\[0.5ex]
White parents & 2 & AAAA \\[0.5ex]
White children & 1 & XXXX \\[0.5ex]
Households & 2 & AAAA \\[0.5ex]
\hline
\end{tabular}
\end{table}

**GENERATION STATS**
Number of male children is 2
Number of female children is 1
Average age of male children is 10.0
Number of male parents is 2
Number of female parents is 2
**MISCELLANEOUS STATS**
Total number of people is 10
Total number of children is 3
Total number of parents is 4
Total number of grandparents is 3
Average household size is 5

From this data, the attacker generates many constraint equations as demonstrated earlier,
and then inputs these constraints into a SAT solver. In this example, we use a free open-source SAT solver
called PicoSAT written in C. Output is given from the Java program Sugar, which wraps PicoSAT for usability improvements.
\newline Output is as follows:
\begin{verbatim}
--Program output and runtime here--
\end{verbatim}

\section{Defending Against a DRA}
As demonstrated above, new techniques must be developed in
order to protect databases against reconstruction attacks, as traditional techniques such as cell suppression are insufficient defense. One of the simplest and most effective techniques in defending against the DRA is noise addition, where the publishing agency adds random values to data before publication in order to increase the size of the solution universe.
For example, if the statistical agency takes a true value of age = 10, then randomly adds either -2, -1, 0, 1, or 2, and then publishes that number, there are now five elements in the solution universe for this value, where without noise addition there would be only one. Adding noise from a more complex distribution such as the Gaussian or Laplace distributions
adds even more elements to the solution universe, since they are not discrete distributions like the example above. Noise addition allows an agency to publish more statistics without fearing that it has given attackers too many constraints to work with.

Especially noteworthy in the discussion of noise addition techniques is a special type of noise addition called the Laplace Mechanism \footnote{More information on the Laplace Mechanism can be found at https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)}.
The mathematics behind this process are beyond the scope of this paper, but the important property of this distribution is that it is effective in creating noise that minimally impacts the utility of the statistics while still ensuring individual privacy. Informally, the proper addition of Laplace noise ensures that the choice of any individual to respond or not respond to a survey does not significantly impact the published data from the survey. Therefore, a person is free to respond truthfully to a survey covered by Laplace noise addition without worrying that his privacy will be compromised by his choice to respond.

\section{Conclusion (needs better title)}

\section{Appendix}

\subsection{SAT and SAT Solvers}

The boolean satisfiability problem, known as SAT, was the first problem to be proven NP-complete.\footnote{Search "Cook-Levin Theorem" for more information.} This problem asks, for a given boolean formula, whether replacing each variable with True or False can make the formula evaluate to True.   A consequence of SAT being NP-complete is that any problem can be reduced in polynomial time (i.e., quickly) to an instance of the SAT problem. Once a problem has been reduced to an instance of the SAT problem, this SAT problem can be fed into a SAT solver to find possible solutions. Although the SAT problem is not solvable by algorithms in polynomial time, researchers have found many heuristic techniques for expediting this process. SAT solvers combine a variety of these techniques into one complex process, resulting in polynomial time solutions for the SAT problem in many cases.

Modern SAT solvers use a heuristic technique called Conflict-Driven Clause Learning, commonly referred to as CDCL. CDCL

There are a wide variety of SAT solvers available to the public for minimal or no cost.

\subsection{Scalability: The Zebra Problem}

\subsection{How Noise Addition Combats a DRA}


\end{document}
