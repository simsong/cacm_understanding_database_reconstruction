\documentclass[jou,apacite]{apa6}
\usepackage{amsmath}

\title{Analysis of a Database Reconstruction Attack on Public Data}
\shorttitle{}

\twoauthors{Christian Martindale}{Simson Garfinkel}
\twoaffiliations{Center for Disclosure Avoidance, U.S. Census Bureau}{Center for Disclosure Avoidance, U.S. Census Bureau}

\abstract{In recent years, a certain type of malicious database attack,
the database reconstruction attack, has become increasingly
more feasible due to rapid advances in attack algorithm
sophistication. Here, we will discuss how these attacks
function, demonstrate their effectiveness and efficiency,
and provide solutions for defending against such an attack.

This paper will answer the following questions:
\begin{enumerate}
    \item What is a database reconstruction attack (DRA)?
    \item To what extent are modern DRAs effective in reconstructing the ground truth data?
    \item How scalable are DRAs with database size?
    \item How can a statistical agency guard public data against a DRA without significantly impacting the accuracy of the data?
\end{enumerate}

We will also develop a vocabulary to facilitate the discussion
of database privacy and formally define some important terms.

\textbf{Keywords: Database Reconstruction Attack, SAT solver, privacy,
disclosure avoidance}}


\begin{document}
\maketitle

\section{Problem Background}
Malicious people often seek to identify individuals from publicly
available data products such as tables and graphs.
Defending against such privacy breaches is
a high priority for statistical agencies, and so researchers
have developed a variety of techniques to prevent database users
from identifying any Personally Identifiable Information (PII)

These techniques include:
\begin{enumerate}
  \item Cell Suppression, where the values of cells with small counts or few possible
        generating combinations are removed from the published table
  \item Row Swapping, where the data rows corresponding to individuals
        with similar values in certain key cells are switched
  \item Bucketing, where numerical values are grouped into
        buckets corresponding to ranges, instead of giving the exact
        values for each entry in the table
  \item Topcoding, where the buckets at the high and low ends
        of the table are given without upper or lower bound (e.g.
        reporting the highest bucket for age as "80+" instead of
        "80-90" and "90-100")
\end{enumerate}

The goal of a Database Reconstruction Attack is to
use public data to create a mathematical system of equations,
which then can be used to reconstruct the original (before the disclosure
avoidance techniques were applied) data set.
While the above techniques are not without merit, this paper will
demonstrate that they alone are insufficient in guarding data against
a modern DRA.

\section{Vocabulary}

\begin{itemize}
\item Database Reconstruction Attack (DRA) - An attempt to determine survey response information from a publicly available data set which has had disclosure avoidance techniques applied to it.

\item Personally Identifiable Information (PII) - Data that can be used to identify a single person.

\item Constraint Equation - A mathematical equation that represents information known to be true for a set of data.

\item Solution Universe (U) - The set of all combinations of a
group of variables that satisfies a certain set of constraint equations.

\item SAT Solver - A program that uses a complex set of heuristics to find the solution universe given a set of constraint equations.

\item Noise Addition - The addition of small random numbers to statistical table values before publication for the purpose of protecting respondent privacy.
\end{itemize}

\section{The Database Reconstruction Attack: An Example}
To understand how a DRA works, let us consider a simple example data table generated from the following survey given to two households:
\begin{verbatim}
Please provide the following for each member
of your family:
Age, Sex, Race, Generation.
\end{verbatim}
Tables 1 and 2 summarize the survey responses for the two households surveyed.
\begin{table}[!htb]
\caption{Household 1}\label{tab1}
\begin{tabular}{cccc}
\hline\\[-1.5ex]
Age & Sex & Race & Gen \\[0.5ex]
\hline\\[-1.5ex]
55 & Female & White & Grandparent\\[0.5ex]
24 & Male & White & Parent\\[0.5ex]
28 & Female & Black & Parent\\[0.5ex]
5 & Female & Black & Child\\[0.5ex]
8 & Male & White & Child\\[0.5ex]
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{Household 2}\label{tab2}
\begin{tabular}{cccc}
\hline\\[-1.5ex]
Age & Sex & Race & Gen \\[0.5ex]
\hline\\[-1.5ex]
50 & Male & White & Parent\\[0.5ex]
45 & Male & White & Parent\\[0.5ex]
15 & Female & White & Child\\[0.5ex]
17 & Female & White & Child\\[0.5ex]
\hline
\end{tabular}
\end{table}

The statistical agency then publishes the following data based
on the survey results: \footnote {Note that statistics formed from groups made up of only one individual are redacted with "XXXX". This is an example of cell suppression used to protect the privacy of those individuals.}

\begin{table}[!htb]
\caption{Survey Results}\label{tab3}
\begin{tabular}{ccc}
\hline\\[-1.5ex]
Group & Number & Average Age \\[0.5ex]
\hline\\[-1.5ex]
Individuals & 9 & 27.44 \\[0.5ex]
Males & 4 & 31.75 \\[0.5ex]
Females & 5 & 24 \\[0.5ex]
Parents & 4 & 36.75 \\[0.5ex]
Grandparents & 1 & XXXX \\[0.5ex]
Children (0-12) & 2 & 6.5 \\[0.5ex]
Children (13-17) & 2 & 16 \\[0.5ex]
Whites & 7 & 30.57 \\[0.5ex]
Blacks & 2 & 24 \\[0.5ex]
Black parents & 1 & XXXX \\[0.5ex]
Black children & 1 & XXXX\\[0.5ex]
White parents & 3 & 39.66 \\[0.5ex]
White children & 3 & 13.33 \\[0.5ex]
Households & 2 & 27.85 \\[0.5ex]
\hline
\end{tabular}
\end{table}

Retabulating these results in a numerical format with the keys in Table 4 gives the results in Table 5.

\begin{table}[!htb]
\caption{Tabulation Key}\label{tab4}
\begin{tabular}{c|c}
\hline\\[-1.5ex]
Key & Value \\[0.5ex]
\hline\\[-1.5ex]
Male & 0 \\[0.5ex]
Female & 1 \\[0.5ex]
White & 0 \\[0.5ex]
Black & 1 \\[0.5ex]
Child & 0 \\[0.5ex]
Parent & 1 \\[0.5ex]
Grandparent & 2 \\[0.5ex]
\hline
\end{tabular}
\end{table}

\begin{table}[!htb]
\caption{Survey Results}\label{tab5}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & 1 & 24 & 0 & 0 & 1  \\[0.5ex]
2 & 1 & 28 & 1 & 1 & 1  \\[0.5ex]
3 & 1 & 55 & 1 & 0 & 2  \\[0.5ex]
4 & 1 & 5 & 1 & 1 & 0  \\[0.5ex]
5 & 1 & 8 & 0 & 0 & 0  \\[0.5ex]
6 & 2 & 50 & 0 & 0 & 1  \\[0.5ex]
7 & 2 & 45 & 1 & 0 & 0  \\[0.5ex]
8 & 2 & 15 & 1 & 0 & 0  \\[0.5ex]
9 & 2 & 17 & 1 & 0 & 0 \\[0.5ex]
\hline
\end{tabular}
\end{table}


The data in Table 5 is the 'ground truth' database that the attacker wishes to reconstruct. At the start of the attack, the attacker writes the following table of 45 unknowns:

\begin{table}[!htb]
\caption{DRA Unknowns}\label{tab6}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & H1 & A1 & S1 & R1 & G1  \\[0.5ex]
2 & H2 & A2 & S2 & R2 & G2  \\[0.5ex]
3 & H3 & A3 & S3 & R3 & G3  \\[0.5ex]
4 & H4 & A4 & S4 & R4 & G4  \\[0.5ex]
5 & H5 & A5 & S5 & R5 & G5  \\[0.5ex]
6 & H6 & A6 & S6 & R6 & G6  \\[0.5ex]
7 & H7 & A7 & S7 & R7 & G7  \\[0.5ex]
8 & H8 & A8 & S8 & R8 & G8  \\[0.5ex]
9 & H9 & A9 & S9 & R9 & G9  \\[0.5ex]
\hline
\end{tabular}
\end{table}

The reconstruction attack works by identifying constraint
equations given by the data table. For example, we have the
following statistic:
\begin{verbatim}
Individuals: 9, 27.44
\end{verbatim}

This can be written as a linear constraint equation: \footnote{In the actual calculation, mathematical transformations must be applied to avoid floating point rounding errors, but that is beyond the scope of this paper.}
\[\frac{100(A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + A9)}{9} = 27.44\]

This equation is in 9 unknowns, and so is unsolvable alone.
However, the attacker can write more constraint equations.

\begin{verbatim}
Grandparents: 1, XXX
\end{verbatim}


Becomes: \footnote{X==Y evaluates to 0 if X!=Y and 1 if $X=Y$.}
\begin{align*}
& (G1==2) + (G2==2) + (G3==2) + (G4==2) + \\
& (G5==2)+ (G6==2) + (G7==2) +\\
& (G8==2) + (G9==2) = 1
\end{align*}

\begin{verbatim}
Children 0-12: 2, 6.5
\end{verbatim}

Becomes the following two equations:

\begin{align*}
& (G1==0) + (G2==0) + (G3==0)+\\
& (G4==0)+  (G5==0) + (G6==0) +\\
& (G7==0) + (G8==0) + G9==0) = 2
\end{align*}
\begin{align*}
& (A1 * (G1==0) + A2 * (G2==0) + A3 * (G3==0) +\\
& A4 * (G4==0) +  A5 * (G5==0) + A6 * (G6==0) + \\
& A7 * (G7==0) + A8 * (G8==0) + A9 * (G9==0)) = 13
\end{align*}
Once the attacker has converted all the published statistics
into equations like the ones above, he will have a system
of a large number of equations in 45 unknowns. This system has one 'true' solution, $S_g$, equivalent to the
ground truth, and possibly many other 'false' solutions, $[S_1...S_n]$.
The Solution Universe, U, is potentially originally quite large, with $U = [S_1...S_n]$

However, each time the agency publishes a new statistic, the attacker
can generate new constraints and therefore narrow
down the set of possible solutions. Eventually, the attacker
will obtain $U = [S_g]$, at which point he has
found the ground truth and reconstructed the database
successfully. Note here that the cell suppression disclosure avoidance technique does not prevent the attacker from performing the reconstruction attack, but rather simply gives him one fewer constraint to work with per cell suppressed.
Even if the number of constraints is insufficient to narrow down U to just one element, the attacker can still often identify personal data because of the high probability that all remaining solutions $S_n$ in $U$ share values for a set of people. For example, if
$U = [S_1, S_2]$ and $S_1$ and $S_2$ both have values $[h_1, a_1, s_1, r_1, g_1]$ in common
(represented mathematically by $S_1 \cap S_2 = [H_1=h_1, A_1=a_1, S_1=s_1, R_1=r_1, G_1=g_1]$),
then we know that $[h_1, a_1, s_1, r_1, g_1]$ must be an actual person in the ground truth set. Thus, the database has failed to protect user privacy even though the attack did not fully reconstruct the database.

\section{Methods of Attack}

The three most common and effective ways of solving a
system of constraint equations are:
\begin{enumerate}

\item \textbf{Brute Force} - trying every possible combination of solutions.\\
\item \textbf{Optimization} - creating a single cost function from the constraints, and then solve the cost function using optimization software. \\
\item \textbf{Using a SAT solver}, which are complex programs that solve Boolean algebra problems.

\end{enumerate}

For large data sets such as the U.S. Census, performing a brute-force attack is infeasible due to the fact that the runtime of brute force programs scales exponentially with the number of unknowns. However, optimizers and SAT solvers are both quite effective because they can reduce the runtime of the program to acceptable (linearly scaling) levels. Recent advances in SAT solver heuristics have enabled these programs to solve systems with even millions of variables in linear time. Although public Census data tables are drawn from hundreds of millions of American individuals, the Census publishes billions of statistics from those individual responses, so there are still sufficient constraints to narrow down the solution universe enormously. In later examples, we will use a SAT solver to demonstrate how quickly these very complex programs can solve large systems of equations.

\section{Revisiting the Example}

In order to show how a SAT solver can be used to rapidly narrow down the solution universe for a data set, we will perform a mock DRA on a new set of responses to the survey given earlier.

The ground truth responses from two new households are given below in Table 7.
\begin{table}[!htb]
\caption{Survey Results}\label{tab7}
\begin{tabular}{c|c|c|c|c|c}
\hline\\[-1.5ex]
ID & Household & Age & Sex & Race & Generation \\[0.5ex]
\hline\\[-1.5ex]
1 & 1 & 80 & 1 & 1 & 2  \\[0.5ex]
2 & 1 & 40 & 0 & 0 & 1  \\[0.5ex]
3 & 2 & 70 & 1 & 0 & 2  \\[0.5ex]
4 & 2 & 30 & 1 & 1 & 1  \\[0.5ex]
5 & 2 & 90 & 0 & 0 & 2  \\[0.5ex]
6 & 3 & 10 & 0 & 1 & 0  \\[0.5ex]
7 & 3 & 10 & 0 & 1 & 0  \\[0.5ex]
8 & 3 & 10 & 1 & 0 & 0  \\[0.5ex]
9 & 4 & 40 & 1 & 0 & 1 \\[0.5ex]
10 & 4 & 20 & 0 & 1 & 1 \\[0.5ex]
\hline
\end{tabular}
\end{table}

The statistical agency publishes the statistics in Table 8 after processing the ground truth responses.

\begin{table}[!htb]
\caption{Data Publication}\label{tab8}
\begin{tabular}{ccc}
\hline\\[-1.5ex]
Group & Number & Average Age \\[0.5ex]
\hline\\[-1.5ex]
Individuals & 10 & 40 \\[0.5ex]
Males & 5 & 34 \\[0.5ex]
Females & 5 & 46 \\[0.5ex]
Parents & 4 & 32.5 \\[0.5ex]
Grandparents & 3 & 80 \\[0.5ex]
Children (total) & 3 & 10 \\[0.5ex]
Children (0-12) & XXXX & XXXX \\[0.5ex]
Children (13-17) & XXXX & XXXX \\[0.5ex]
Whites & 5 & 50 \\[0.5ex]
Blacks & 5 & 30 \\[0.5ex]
Black parents & 2 & AAAA \\[0.5ex]
Black children & 2 & AAAA\\[0.5ex]
White parents & 2 & AAAA \\[0.5ex]
White children & 1 & XXXX \\[0.5ex]
Households & 2 & AAAA \\[0.5ex]
\hline
\end{tabular}
\end{table}

From this data, the attacker generates many constraint equations as demonstrated earlier,
and then inputs these constraints into a SAT solver. In this example, we use a free open-source SAT solver
called PicoSAT written in C. Output is given from the Java program Sugar, which wraps PicoSAT for usability improvements.
\newline Output is as follows:
\begin{verbatim}
--Program output and runtime here--
\end{verbatim}

\section{Defending Against a DRA}
As demonstrated above, new techniques must be developed in
order to protect databases against reconstruction attacks, as traditional techniques such as cell suppression are insufficient defense. One of the simplest and most effective techniques in defending against the DRA is noise addition, where the publishing agency adds random values to data before publication in order to increase the size of the solution universe. For example, if the statistical agency takes a true value of age = 10, then randomly adds either -2, -1, 0, 1, or 2, and then publishes that number, there are now five elements in the solution universe for this value, where without noise addition there would be only one. Adding noise from a more complex distribution such as the Gaussian or Laplace distributions adds even more elements to the solution universe, since they are not discrete distributions like the example above. Noise addition allows an agency to publish more statistics without fearing that it has given attackers too many constraints to work with.

Especially noteworthy in the discussion of noise addition techniques is a special type of noise addition called the Laplace Mechanism \footnote{More information on the Laplace Mechanism can be found at https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)}.
The mathematics behind this process are beyond the scope of this paper, but the important property of this distribution is that it is effective in creating noise that minimally impacts the utility of the statistics while still ensuring individual privacy. Informally, the proper addition of Laplace noise ensures that the choice of any individual to respond or not respond to a survey does not significantly impact the published data from the survey. Therefore, a person is free to respond truthfully to a survey covered by Laplace noise addition without worrying that his privacy will be compromised by his choice to respond.

\section{Conclusion (needs better title)}

\section{Appendices}

\subsection{Appendix 1: SAT and SAT Solvers}

The boolean satisfiability problem, known as SAT, was the first problem to be proven NP-complete.\footnote{Search "Cook-Levin Theorem" for more information.} This problem asks, for a given boolean formula, whether replacing each variable with True or False can make the formula evaluate to True.   A consequence of SAT being NP-complete is that any problem can be reduced in polynomial time (i.e., quickly) to an instance of the SAT problem. Once a problem has been reduced to an instance of the SAT problem, this SAT problem can be fed into a SAT solver to find possible solutions. Although the SAT problem is not solvable by algorithms in polynomial time, researchers have found many heuristic techniques for expediting this process. SAT solvers combine a variety of these techniques into one complex process, resulting in polynomial time solutions for the SAT problem in many cases.

Modern SAT solvers use a heuristic technique called Conflict-Driven Clause Learning, commonly referred to as CDCL. CDCL, at a high level, works by the following process:

\begin{enumerate}

\item Assign a value to a variable arbitrarily.
\item Use this assignment to determine values for the other variables in the formula (a process known as unit propagation).
\item If a conflict is found, backtrack to the clause that made the conflict occur and undo variable assignments made after that point.
\item Add the negation of the conflict-causing clause as a new clause to the master formula and resume from step 1.

\end{enumerate}

This process is much faster at solving SAT problems than previous processes used in SAT solvers because adding conflicts as new clauses has the potential to avoid wasteful 'repeated backtracks'.

There are a wide variety of SAT solvers available to the public for minimal or no cost. Little technical computer skill or mathematical knowledge is required to use a modern SAT solver, so the only issue that must be overcome is determining how to translate the problem into a boolean formula. Programs such as Naoyuki Tamura's Sugar make the use of SAT solvers even easier by allowing the user to input mathematical and even English constraints into a SAT solver, removing the often laborious work of translating complex constraints into boolean formulae.

\subsection{Appendix 2: Scalability and The Zebra Problem}

To demonstrate the real capabilities of the SAT solver when faced with a very large constraint system, in contrast to our above relatively small examples, we will use a system with 155 variables and 1135 boolean clauses. This system results from the CNF encoding of a famous problem known as the Zebra Problem \footnote{For more information: http://www.ics.uci.edu/~csp/r8.pdf}.
This problem is appropriate because its structure closely models the current Census data collection and publication process. The writer receives the survey answers from the respondents, redacts much of the information to avoid giving away PII, and then publishes a data product.

The size of this problem more closely approximates the number
of variables an attacker would need to generate constraints for when performing a DRA through queries on a massive public data set. It is too complex for the vast majority of humans to solve in any reasonable time.

This problem is stated as follows:

\begin{enumerate}
\item Five people have five different pets, smoke five different
       brands of cigarettes, have five different favorite drinks and
       live in five different houses.
\item The Englishman lives in the red house.
\item The Spaniard has a dog.
\item The Ukranian drinks tea.
\item The Norwegian lives in the leftmost house.
\item The Japanese smokes Parliaments.
\item The Norwegian lives next to the blue house.
\item Coffee is drunk in the green house.
\item The snail owner smokes Old Gold.
\item The inhabitant of the yellow house smokes Kools.
\item The Lucky Strikes smoker drinks orange juice.
\item Milk is drunk in the middle house.
\item The green house is immediately to the right of the ivory house.
\item The Chesterfield smoker lives next door to the fox owner.
\item The Kools smoker lives next door to where the horse is kept.
\end{enumerate}


\begin{itemize}
\item Given these conditions, determine who owns the zebra and who drinks water.
\item (The attacker is 'targeting' the water drinker and the zebra owner.)
\end{itemize}

Just as demonstrated in the previous example, this problem
can be encoded as a large set of boolean equations. The problem
gives 15 explicit constraints which allow the attacker to generate more constraints, and each of the combinations of variables (five people, five houses, five drinks, etc.) can be represented as a boolean variable, for example, "The Norwegian lives in the red house == False"

Once the attacker has generated every constraint possible from the given information, he can input the constraints into a SAT solver. In this example, we run the zebra problem through PycoSAT, a free Python wrapper of PicoSAT.

The results:

\begin{verbatim}
Solving for all solutions...
Solution:  [-1, -2, 3, -4, -5, -6,
-7, -8, -9, 10, 11, -12, -13, -14,
-15, -16, -17, -18, 19, -20, -21,
22, -23, -24, -25, -26, -27, -28,
-29, 30, 31, -32, -33, -34, -35,
-36, -37, -38, 39, -40, -41, 42,
-43, -44, -45, -46, -47, 48, -49,
-50, 51, -52, -53, -54, -55, -56,
57, -58, -59, -60, -61, -62, -63,
64, -65, -66, -67, 68, -69, -70,
-71, -72, -73, -74, 75, 76, -77,
-78, -79, -80, -81, 82, -83, -84,
-85, -86, -87, 88, -89, -90, -91,
-92, -93, 94, -95, -96, -97, -98,
-99, 100, -101, -102, -103, -104,
105, 106, -107, -108, -109, -110,
-111, -112, -113, 114, -115, -116,
-117, 118, -119, -120, -121, 122,
-123, -124, -125, -126, -127, 128,
-129, -130, -131, -132, -133, -134,
-135, -136, 137, -138, 139, -140,
-141, 142, -143, -144, -145, -146,
-147, 148, 149, -150, 151, -152,
-153, 154, -155]
Number of solutions found in 0.00046s :  1
\end{verbatim}

Each of the 155 numbers represents one of the variables encoded in the formula input to the SAT solver.
Positive integers correspond to True, while negative integers are False. For example, $118$ in the solution output means that the boolean variable "The Spaniard drinks orange juice" is True.
PicoSAT is able to solve this very complex problem in a fraction of a second, and has reconstructed the entire database, thus obtaining the PII of all the residents of the village despite the fact that the vast majority of their information was redacted by the problem writer before release.

The success of this attack demonstrates the inefficacy of
cell suppression as a method for protecting PII against
a SAT solver-driven DRA. This problem
contained far more suppressed cells than unsuppressed ones, which is not realistic given that a real publishing agency would want to minimize the number of suppressed cells. Despite this over-suppression, the SAT solver was still able to easily solve for the values of all cells in the example.


\begin{thebibliography}{9}
\bibitem{latexcompanion}
Michel Goossens, Frank Mittelbach, and Alexander Samarin.
\textit{The \LaTeX\ Companion}.
Addison-Wesley, Reading, Massachusetts, 1993.


\end{thebibliography}

Naoyuki Tamura, Akiko Taga, Satoshi Kitagawa, Mutsunori Banbara: Compiling Finite Linear CSP into SAT, Constraints, Volume 14, Number 2, pp.254--272, June, 2009.

\end{document}
