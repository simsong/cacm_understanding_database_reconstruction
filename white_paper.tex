\documentclass[runningheads]{llncs}
\include{vars}
\newif\ifanonymized
\anonymizedtrue
\newif\ifshortversion
\shortversiontrue
\newif\iflongversion
\longversionfalse
%\anonymizedfalse
% cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[ngerman,english]{babel}
\usepackage{verbatim}
%better font, similar to the default springer font
%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%Sorts the citations in the brackets
%It also allows \cite{refa, refb}. Otherwise, the document does not compile.
%  Error message: "White space in argument"
\usepackage{cite}
%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}
%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

%tweak \url{...}
\usepackage{url}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
%required for pdfcomment later
\usepackage{xcolor}
%enable nice comments
%this also loads hyperref
\usepackage{pdfcomment}
%enable hyperref without colors and without bookmarks
\hypersetup{hidelinks,
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with packages todo, easy-todo, todonotes
\newcommand{\todo}[1]{\commentatside{#1}}
%compatiblity with package fixmetodonotes
\newcommand{\TODO}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%% END COPYING HERE

% Two kinds of cite, for the long version and the short version
\ifshortversion
\newcommand{\citelong}[1]{}
\else
\newcommand{\citelong}[1]{\cite{#1}}
\fi

\begin{document}
\title{Database Reconstruction Attack on Public Data}
\titlerunning{Database Reconstruction}
\ifanonymized
\author{Anonymized Author(s)}
\institute{Institute for Anonymous Papers}
\else
\author{Christian Martindale \and Simson Garfinkel}
\institute{Center for Disclosure Avoidance, U.S. Census Bureau}
\fi

\maketitle
\begin{abstract}
Statistical agencies are mandated to publish summary statistics and
micro-data while not providing data users with the ability to derive
specific attributes for particular individuals or
establishments. 
\ifshortversion
\else
Today these privacy guarantees are assured through the
use of \emph{Statistical Disclosure Limitation} (SDL)
techniques, such as suppressing values in certain cells 
and/or generalizing (coarsening) categories in
the summary tables. 
\fi
\ifshortversion
Traditional statistical disclosure limitation (SDL) techniques
\else
Although these techniques 
\fi
are not sufficient to
prevent a database reconstruction attack of the sort anticipated by
Dinur and Nissim\cite{noise}, 
\ifshortversion
and
\else
\fi
adopt disclosure limitation techniques based on formal privacy
techniques such as differential privacy. 
\ifshortversion
\else
Slow adoption may have been due to 
the belief that current SDL practices, which include random noise infusion,
are sufficient to protect against feasible reconstruction attacks, the absence
of off-the-shelf formal privacy software, and the scarcity of expertise on
these systems within statistical agencies. 
\fi
This paper shows how a database
reconstruction attack functions and demonstrates their effectiveness
and efficiency on a toy example. We then show how SDL techniques based
on additive input and output noise infusion can be effective at defending 
against database reconstruction.
\end{abstract}

\begin{keywords}
database reconstruction attack, SAT solver, 
\ifshortversion\else privacy,\fi 
disclosure avoidance
\end{keywords}


\section{Introduction}
Working Paper \#22 of the U.S. Federal Committee on Statistical
Methodology\cite{workingpaper22} outlines the currently accepted best
practices for U.S. statistical agencies to follow when they prepare and
release both statistical data and ``de-identified''
micro-data. Broadly, statistical agencies are charged with releasing
high-quality data to further public policy goals, but they are
prohibited from releasing statistics or micro-data that might result in the
identification of data about a specific individual or establishment, or the linkage
of micro-data to a responding entity.

Working Paper \#22 outlines a number of approaches that statistical
agencies can use for protecting respondent data. These techniques include:
\begin{enumerate}
  \item \textbf{Cell Suppression}, where the values of certain  cells with small counts or few possible
        generating combinations are removed from the published table
  \item \textbf{Row Swapping}, where the data rows corresponding to individuals
        with similar values for certain key cells are switched
  \item \textbf{Generalization}, where numerical values are grouped into
        buckets corresponding to ranges, instead of giving the exact
        values for each entry in the table, also called \emph{coarsening}
  \item \textbf{Top-and-bottom-coding}, where the statistical groups at the high and low ends
        of the table are given without upper or lower bound (e.g.
        reporting the highest group for age as 80+ instead of
        80-90 and 90-100), a form of coarsening
\end{enumerate}

While it makes intuitive sense that these techniques hamper the
ability of a  \emph{data intruder}\citelong{data-intruder} to recover respondent data from the
statistical release, such hunches do not constitute rigorous
mathematical proofs. Absent a formal definition of privacy and
mathematical proofs showing that a specific disclosure limitation
technique realizes that definition, there is no way to know if
techniques actually protect privacy, or if that is merely wishful thinking.

In this paper, we use
the term \emph{database reconstruction attack} (DRA) to describe the process of
taking a published set of statistical tables and deriving the
underlying sensitive record-level data. Such attacks are possible and surprisingly
practical, and this fact has been known for more than 15
years\cite{noise}. Yet surprisingly, today most of
statistical agencies still rely on the disclosure
limitation techniques described in Working Paper \#22, rather than
having adopted new techniques based on differential
privacy\cite{Dwork:2006:CNS:2180286.2180305}, or other formal privacy systems 
that operate with similar principles\cite{KiferMachanavajjhala:2012}.

The contribution of this paper is to present a simple example of how
a database reconstruction attack might be implemented against a data
release from an official statistics agency, and then to show how a
formally private technique can protect against such an
attack. Although the possibility of database reconstruction is
presented in Dinur and Nissum's original paper\cite{noise}, we are not
aware of any \iflongversion end-to-end \fi example showing the threat, a worked attack,
and the results of modern defenses.

\section{Related Work}

Dinur and Nissim\cite{noise} showed that the underlying
confidential data of a statistical database can be reconstructed with
a surprisingly small number of queries. In practice, most statistical
agencies perform these queries themselves when they release
statistical tables. Thus, Dinur and Nissim's primary finding
is that a statistical agency that publishes too many accurate statistical
tables drawn from the same confidential dataset risks inadvertently
compromising that dataset unless it takes specific protective measures.

Statistical tables create the possibility of database reconstruction
because they form a set of constraints for which there is ultimately
only one exact solution. Restricting
the number or specific types of queries---for example, by suppressing
results from a small number of respondents---is often insufficient to prevent access
to indirectly identifying information, because the system's refusal to
answer a ``dangerous'' query itself provides the attacker with information. 
Dinus and Nissim found that if a database is modeled as a string of $n$ bits,
then at least $\sqrt{n}$ bits must be modified by adding noise to
protect individuals from being identified.

Kasiviswanathan, Rudelson, and Smith\cite{Kasiviswanathan:2013:PLR:2627817.2627919} introduced
the concept of the linear reconstruction attack, which underlies the  DRA. The key concept is that,
given nonsensitive data such as zip code or gender, an attacker
can construct a matrix of linear equalities that can often be solved
in polynomial time. \iflongversion The paper also analyzes a common reconstruction
technique known as least squares decoding, where the attacker sets up
a goal function to minimize the square of the distance between two
databases in order to reconstruct the original database.\fi

\iflongversion
Brown and Heathers\cite{doi:10.1177/1948550616673876} developed the
granularity-related inconsistency of means (GRIM) test in response to
observed inconsistencies in published data from psychological
journals. This test is centered around the premise that, for
statistics drawn from integer data, only certain means are
possible. The GRIM test determines whether reported means could
possibly have come from data sets with a certain size, granularity,
and group number. In surveying 71 published articles, the authors
found 36 papers with one inconsistency and 16 with two or more
inconsistencies. Although this test was intended to detect possible
errors or mean falsification in published articles, the concept of
drawing inferential conclusions about a data set based only on 
published statistics is a key concept behind the DRA.
\fi

\section{The Database Reconstruction Attack: An Example}

To present a DRA, we consider a hypothetical
census of a fictional block conducted by a fictional statistical
agency. For every household, the agency collects each resident's age,
sex, race, and their generation in the household.  Because the
confidential data will not be released in order to protect respondent privacy, the statistical agency
publishes every conceivable statistic that might be of use to a
potential demographers, sociologists and economists. Based on user
feedback\iflongversion from previous census reports\fi, the agency has chosen 
a set of statistics, which we present here as Table~\ref{publishedstatsbig}.

We assume that the agency applied SDL using only primary cell suppression---deleting items
in cells with size one (singletons). Normally, an agency would also apply complementary 
suppression--deleting additional items to insure that the singleton cells
could not be recovered by subtraction\cite{Fellegi:1972}. Doing so in this
example would have eliminated most of the published data. Our example can be
extended to larger collections of tables where both types of suppression can 
be done.

\ifshortversion
\newcommand{\RC}[1]{#1}
\else
\newcommand{\RC[1]{}}
\fi


\newcommand{\censored}{\multicolumn{1}{c||}{\rule{6mm}{3mm}}}
\newcommand{\censoredb}{\multicolumn{1}{c}{\rule{6mm}{3mm}}}
\begin{table}
\begin{center}
\caption{Fictional statistical data published by the fictional
  statistics agency. Item numbers are for identification purpose
  only. Note that statistics 2D, 4B, 4E and 5D have been suppressed as part
  of the statistical disclosure limitation process.
\ifshortversion
The rightmost column, ``Average Age Protected,'' shows the value of Average Age protected
with the addition of Laplace noise sufficient to prevent a database reconstruction attack,
as discussed in section~\ref{solution}
\fi
}\label{publishedstatsbig}
\ifshortversion
\begin{tabular}{l|l|c|c||c}
\else
\begin{tabular}{l|l|c|c|}
\fi

\RC{ &       &        &                 & Average Age\\}
Item & Group & Number & Average Age \RC{& Protected}\\
\hline
1A & Individuals & 10 & 40 \RC{&38.57} \\
1B & Males       & 5 & 34  \RC{&33.21} \\
1C & Females     & 5 & 46  \RC{&43.92} \\
1D & Whites      & 5 & 50  \RC{&47.82} \\
1E & Blacks      & 5 & 30  \RC{&29.31} \\
\hline
2A & Children (0-17) & 3 & 10 \RC{&8.50} \\
2D & White children  & 1 & \censored \RC{& \censoredb} \\
2E & Black children  & 2 & 10 \RC{8.75} \\
\hline
3A & Parents         & 4 & 32.5 \RC{&31.41} \\
3B & Male parents    & 2 & 30   \RC{&29.77} \\
3C & Female parents  & 2 & 35   \RC{&33.06}\\
3F & Parents over 40 & 0 & --   \RC{&--}\\
\hline
4A & Grandparents        & 3 & 80        \RC{& 78.18} \\
4B & Male grandparents   & 1 & \censored \RC{& \censoredb} \\
4C & Female grandparents & 2 & 75        \RC{&72.15}  \\
4D & White grandparents  & 2 & 80        \RC{&77.64}  \\
4E & Black grandparents  & 1 & \censored \RC{& \censoredb} \\
\hline
5A & Households                  & 2 & 40 \RC{&38.57} \\
5B & Tri-generational households & 0 & -- \RC{&--} \\
5C & Single-parent households    & 0 & -- \RC{&--} \\
5D & Childless households        & 1 & \censored \RC{& \censoredb} \\
% Remove these constraints because they aren't needed to reconstruct 
% Everything except the children.
% 5E & Interracial married couples & 2 & 32.5 \\
% 5F & Same-sex married couples & 0 & -- \\
% 5G & Households $\geq 40\% $ female & 2 & 40 \\
% 5H & Households $\geq 40\% $ black & 2 & 40 \\
\hline
\end{tabular}
\end{center}
\end{table}

The goal of the DRA is to reconstruct the number of households and,
for each household, to learn the age, sex, race, and generation of
each member. To do this, the data intruder views these attributes as a set of five
unknown variables. Since there are 10 individuals in
the fictitious census, there are 50 unknowns (Table~\ref{50unknowns}).

\begin{table}
\caption{Unknowns for the 10 fictitious individuals whose statistical
  data are presented in Table~\ref{publishedstatsbig}. Unknowns
  H\textit{n} are the household, A\textit{n} the age, S\textit{n} the
  sex, R\textit{n} the race and G\textit{n} the generation. Sex, Race
  and Generation categorical values; they are converted into numerical
  values using the key at the right.}\label{50unknowns}
\begin{minipage}[t]{.6\textwidth}
~\\[0pt]                        % aligns minipages for some reason
  \input{id_table.tex}
\end{minipage}
\hfill
\begin{minipage}[t]{.35\textwidth}
~\\[0pt]                        % aligns minipages for some reason
  \begin{tabular}{c|c}
  Key & Value \\
  \hline
  \hline
  Male & 0 \\
  Female & 1 \\
  \hline
  White & 0 \\
  Black & 1 \\
  \hline
  Child & 0 \\
  Parent & 1 \\
  Grandparent & 2 \\
  \hline
  \end{tabular}
\end{minipage}
\end{table}

The data intruder proceeds by examining the published tables and
translating the published statistics into constraints on the 
record-level private
data. The constraints are expressed as mathematical formulae
representing rules that the ground truth must satisfy. Below we
identify how specific released statistics can be mapped to
constraints.

We can encode the constraint based on statistic 1A as a linear
constraint equation with 10 unknowns, each of which has the range
$0\ldots120$, using the symbols for age $A1...A10$
(Table~\ref{50unknowns}), where $=$ is the constraint operator
establishing that the two sides must be equal:

\begin{equation}\label{eq1}
\frac{A1 + A2 + A3 + A4 +...+ A10}{10} = 40
\end{equation}

Two constraints can be encoded based on statistic 1B: the existence of
five men, and the average age of 34. To encode these kinds of
conditional constraints, we must introduce the == operator,
which here is a test for equivalence. This operator returns 1 if the
left and right hand sides are equal, and 0 otherwise.
The existence of five men in the survey is thus coded as:

\begin{equation}
\begin{split}
(S1==0) + (S2==0) + (S3==0) + (S4==0) + (S5==0) +  & \\
(S6==0) + (S7==0) + (S8==0) + (S9==0) + (S10==0) & = 5
\end{split}
\end{equation}

And five men with the average age of 34 is:

\begin{equation}
\begin{split}
A1\times(S1==0) + A2\times(S2==0) + A3\times(S3==0) + &\\
A4\times(S4==0) + A5\times(S5==0) + A6\times(S6==0) + &\\
A7\times(S7==0) + A8\times(S8==0) + A9\times(S9==0) + &\\
                                    A10\times(S10==0) & = 34\times5
\end{split}
\end{equation}

Once the attacker has converted all the published statistics
into equations like the ones above, the equations together form a
system of many equations in 50 unknowns.
There is a \textit{solution universe} of all possible solutions to
this set of simultaneous equations. If there is a single possible
solution, then the published statistics completely reveal the
underlying confidential data---the \emph{ground truth}. However, if
there is more than one possible solution, then one of those solutions
is correct, and the others are not. If the equations have no solution,
then the set of published statistics is inconsistent.

\section{Performing the Attack}
A straightforward but possibly inefficient approach to solving this
system of simultaneous equations is to perform a brute force
search---that is, to try all possible values of all possible variables
and record the ones that work.

Ordinarily a brute force search on even this toy problem would be
unreasonable: there are 10 age variables that can range from 0,...,120,
30 binary variables (household, sex and race), and ten trinary
variables. All together, there are $120^{10} \times 2^{30} \times
3^{10} \approx 4 \times 10^{34}$ possible combinations. Even if we
could try a billion a second, trying all possible combinations would
take a billion billion years.

Those familiar with complexity theory will have recognized by now that
the system of variables can be expressed as a satisfiability (SAT)
problem of purely Boolean variables. This is done by encoding each of the
integer and trinary variables as themselves being derived
from a set of Boolean variables with still more constraints. Encoding
the DRA as a SAT problem allows it to be solved using a SAT solver, which are 
programs that have been developed over the past two
decades that use sophisticated heuristics that can rapidly solve many
SAT problems.

Those not familiar with modern SAT solvers may be somewhat
incredulous at trying to  solve
a problem with a work factor of roughly $10^{34}\approx2^{113}$.
However, ``The past few years have seen an enormous progress in the performance
of Boolean satisfiability (SAT) solvers. Despite the worst-case
exponential run time of all known algorithms, satisfiability solvers
are increasingly leaving their mark as a general-purpose tool in areas
as diverse as software and hardware verification,
automatic test pattern generation, planning,
scheduling, and even challenging problems from algebra. Annual SAT
competitions have led to the development of dozens
of clever implementations of such solvers, an exploration of many new
techniques, and the creation of an extensive suite of real-world
instances as well as challenging hand-crafted benchmark
problems. Modern SAT solvers provide a ``black-box'' procedure that
can often solve hard structured problems with over a million variables and
several million constraints.''\cite[references omitted]{Gomes200889}.

Many SAT solvers take their input in the so-called DIMACS file format,
which specifies a single equation in conjunctive normal form (CNF). We use
Sugar\cite{sugar} to transform our system of constraints into DIMACS
format. Surgar's input is a series of
\textit{s-expressions}\citelong{McCarthy:1960:RFS:367177.367199}. For
example, the constraint in Equation~\ref{eq1} can be encoded as the following
s-expression:
\begin{equation}
\texttt{(= (+ A1 A2 A3 A4 A5 A6 A7 A8 A9 A10) 400)}
\end{equation}

Notice that we do not encode the division operator. Because SAT
solvers can only perform operations on Boolean values, an integer division
operator would not yield an exact solutions due to integer round-off error.

Our source input file consists of \NumSExpressions s-expressions in \NumConstraintLines{} lines (minus comments).\footnote{The entire file can be downloaded from our website \emph{ANONYMIZED}.}
This converts into
\NumVariables Boolean variables and \NumClauses CNF clauses. Using the
open source SAT solver PicoSAT\cite{Biere_picosatessentials}, we are able
to solve this system in 11.86 seconds on a 2013 MacBook Pro with a 2.8GHz Intel
i7 processor and 16GiB of RAM (although the program was not limited by RAM). Output is given in Table~\ref{sugarbig} and is tabulated for readability and sorted by household.

Examining the output, we see that 8 of the 10 fictional respondents are exactly solved, while the remaining two are each off by 4 years (one 4 years too high, the other 4 years too low, so as the average age constraint is maintained). In this case there are a number of permutations of children's ages that satisfy the constraints. We could eliminate this by adding more constraints. Indeed, size of the solution set can be experimentally determined by adding additional constraints to the mix: in this case, for example, the constraint that the children may not have the ages of 6, 10 and 14. 

%At past international SAT solver competitions, 
SAT solvers such as PicoSAT can solve problems
with tens of millions of variables in less than 20 minutes
\cite{satcomp}, making it likely that today's SAT solvers could solve
realistic database reconstruction problems based on data being
published today by official statistics agencies.


\begin{table*}
\begin{minipage}[t]{.45\linewidth}
~\\
\caption{Sugar output when run on the encoded statistics in Appendix 3}\label{sugarbig}
\input{id_table_solved.tex}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\linewidth}
~\\
\caption{Responses from a two fictional households
for a survey that collects Age, Sex, Race and Generation of each resident. This is
the unpublished, confidential data collected by a fictional statistical
agency.}\label{responses}
\begin{tabular}{rllp{1in}}
\hline
\multicolumn{4}{c}{Household \#1}   \\
Age & Sex & Race & Generation     \\
\hline
90  & 0 Male   & 0 White & 2 Grandparent   \\      
80  & 1 Female & 1 Black & 2 Grandparent \\    
70  & 1 Female & 0 White & 2 Grandparent \\  
40  & 0 Male   & 0 White & 1 Parent        \\
30  & 1 Female & 1 Black & 1 Parent      \\
\hline
\\
\multicolumn{4}{c}{Household \#2} \\
Age & Sex & Race & Generation     \\
\hline
40 & 1 Female & 0 White & 1 Parent      \\
20 & 0 Male   & 1 Black & 1 Parent        \\
10 & 1 Female & 0 White & 0 Child       \\
10 & 0 Male   & 1 Black & 0 Child         \\
10 & 0 Male   & 1 Black & 0 Child         \\
\hline
\end{tabular}
\end{minipage}
\end{table*}



\section{Defending Against a DRA}\label{solution}
There are three approaches for defending against a DRA: publish less
statistical data, and apply noise (randomness) to either the
statistical data being tabulated (input noise infusion in SDL), or to the results of those
tabulations (output noise infusion). We consider them in order below.

Although it might seem that publishing less statistical data is a
reasonable defense against the DRA, this may severely limit the number
of tabulations that can be published with this approach. A
related problem is that it may be computationally infeasible to
determine if a reconstruction is possible, a direct result of DRA being an NP-complete
problem. \iflongversion That is, the inability of a SAT
solver to find a solution to a SAT problem does not mean that a
solution is not possible---it may just mean that the attacker hasn't waited
long enough.\fi

A second approach is to apply noise to the data before
tabulation. For example, each respondent's age might be randomly altered by $-2 \ldots 2$; 
the statistical agency then tabulates the values. 
In this scenario, if attacker performs a DRA, the result is the altered dataset, and not the underlying true values. Thus, if two responses
are processed with noise of this sort, there are now $5^2 = 25$
possible solutions. If 10 people undergo noise infusion of this type,
there are $5^{10}$---nearly 10 million---possible
solutions. Database reconstruction may still be possible, but there is no
way to know which solution is correct.

A third approach is to use output noise infusion, for example, using
the Laplace mechanism\cite{Dwork:2006:CNS:2180286.2180305}. Output
noise infusion has the advantage of being easier to apply, but the resulting tables will not be internally consistent.
\ifshortversion
The rightmost column of Table~\ref{publishedstatsbig} shows the results of output noise infusion.
\else
\\
For example, in Table~\ref{publishedstatsnoise} we present the results
of the original Table~\ref{publishedstatsbig}, but with Laplace noise
added.  
\fi
With output noise infusion, the attacker must revise each constraint from an equality to a bounded range, resulting in an explosion of possible solutions that is exponential in the number of constraints. Exact solutions can still be found, but there is no way for an attacker to determine which are correct
 Like input noise infusion, output noise infusion does not prevent an attacker from reconstructing the database, but it prevents the attacker from knowing if the reconstruction is correct. .

\iflongversion
\begin{table}[t]
\begin{tabular}{c|c|c}
Group & Number & Average Age \\
\hline
Individuals & 10 & 38.57 \\
Males & 5 & 33.21 \\
Females & 5 & 43.92 \\
Whites & 5 & 47.82 \\
Blacks & 5 & 29.31 \\
\hline
Children (0-12) & 3 & 8.50 \\
White children & 1 & \multicolumn{1}{c}{\rule{6mm}{3mm}} \\
Black children & 2 & 8.75 \\
\hline
Parents & 4 & 31.41 \\
Male parents & 2 & 29.77 \\
Female parents & 2 & 33.06 \\
Parents over 40 & 0 & -- \\
\hline
Grandparents & 3 & 78.18 \\
White grandparents & 2 & 77.64 \\
Black grandparents & 1 & \multicolumn{1}{c}{\rule{6mm}{3mm}} \\
Male grandparents & 1 & \multicolumn{1}{c}{\rule{6mm}{3mm}} \\
Female grandparents & 2 & 72.76 \\
\hline
Households & 2 & 38.57 \\
Tri-generational households & 0 & -- \\
Single-parent households & 0 & -- \\
Childless households & 1 & \multicolumn{1}{c}{\rule{6mm}{3mm}} \\
% Interracial married couples & 2 & 31.41 \\
% Same-sex married couples & 0 & -- \\
% Households $\geq 40\% $ female & 2 & 38.57 \\
% Households $\geq 40\% $ black & 2 & 38.57 \\

\hline
\end{tabular}
\caption{Data publication with added Laplace noise}\label{publishedstatsnoise}
\end{table}
\fi

\section{Conclusion}

With the dramatic improvement in the efficiency of SAT solvers in the
last decade, database reconstruction attacks are no longer just a 
theoretical danger. The vast quantity of data products published by
statistical agencies each year may give a determined attacker 
more than enough constraints to reconstruct some or all of a target database and
breach the privacy of millions of people. Although a suite of traditional
disclosure avoidance technique is often sufficient to defend against a cursory attack, cell suppression and generalization are not secure against more sophisticated
forms of attack. Agencies may also be using input and output noise infusion, but 
current systems may not be noisy enough to materially control the DRA risk.
Formal privacy methods, especially differential privacy, were specifically
designed to to control this risk and, as both of our noise-infusion examples
illustrate, they do so by systematically expanding the universe of solutions
to the DBA constraints. In this expanded universe, the real confidential data
are but a single solution, and no evidence in the published data can improve 
an attacker's guess about which solution is the correct one.

\iflongversion \section{References}\fi

\bibliographystyle{splncs03}
\bibliography{white_paper}
\iflongversion
\newpage
\emph{These appendices will not be part of the FC2018 submission, but
  would be part of a technical report.}

\section{Appendices}

\subsection{SAT and SAT Solvers}

The Boolean satisfiability problem (SAT) was the first
problem to be proven NP-complete\citelong{cooklevin}. This problem asks,
for a given Boolean formula, whether replacing each variable with
either True or False can make the formula evaluate to True.  A
consequence of SAT being NP-complete is that any problem can be
reduced in polynomial time (i.e., quickly) to an instance of the SAT
problem. Once a problem has been reduced to an instance of the SAT
problem, this SAT problem can be fed into a SAT solver to find
possible solutions. Although the SAT problem is not solvable by
algorithms in polynomial time, researchers have found many heuristic
techniques for expediting this process. SAT solvers combine a variety
of these techniques into one complex process, resulting in polynomial
time solutions for the SAT problem in many cases.

Modern SAT solvers use a heuristic technique called Conflict-Driven
Clause Learning, commonly referred to as CDCL\cite{cdcl}. Briefly, CDCL algorithm
is:

\begin{enumerate}

\item Assign a value to a variable arbitrarily.
\item Use this assignment to determine values for the other variables
  in the formula (a process known as unit propagation).
\item If a conflict is found, backtrack to the clause that made the
  conflict occur and undo variable assignments made after that point.
\item Add the negation of the conflict-causing clause as a new clause
  to the master formula and resume from step 1.

\end{enumerate}

This process is much faster at solving SAT problems than previous
processes used in SAT solvers because adding conflicts as new clauses
has the potential to avoid wasteful ``repeated backtracks.''
Additionally, CDCL and its predecessor algorithm, DPLL, are both
provably complete algorithms and will always return either a solution
or ``Unsatisfiable'' if given enough time and memory.

There are a wide variety of SAT solvers available to the public for
minimal or no cost. Although a SAT solver requires the user to
translate the problem into Boolean formulae before use, programs such
as Naoyuki Tamura's Sugar facilitate this process by translating
user-input mathematical and English constraints into Boolean formulae
automatically.

\subsection{Sugar Input}

Sugar input is given in a standard Constraint Satisfaction Problem
(CSP) file format. A constraint must be given on a single line of the
file, but here we separate most constraints into multiple lines for
readability. Constraint equations are separated by comments describing
what statistics they encode.

Input for the model in this paper is as follows:

\verbatiminput{constraints.csp}
\fi

\end{document}

% LocalWords:  microdata equalities satisfiability
