\documentclass[runningheads]{llncs}
\include{vars}
%\newif\ifanonymized
%\anonymizedtrue
%\newif\ifshortversion
%\shortversionfalse
%\newif\iflongversion
%\longversiontrue

%\anonymizedfalse
%\shortversionfalse
%\longversiontrue
% cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[ngerman,english]{babel}
\usepackage{fancyvrb}
\usepackage{cite}
\usepackage{paralist}  % extended enumerate, such as \begin{compactenum}
%\usepackage{csquotes}  % for easy quotations: \enquote{text}
%\usepackage{microtype} % enable margin kerning
\usepackage{url}       % provides \url{...}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\usepackage{xcolor}      % required for pdfcomment later
\usepackage{pdfcomment}  % enable nice comments, also loads hyperref
\hypersetup{hidelinks,   % enable hyperref without colors and without bookmarks
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
\usepackage[all]{hypcap} %enables correct jumping to figures when referencing

\newcommand{\commentontext}[2]{%
  \colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{%
  \pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

\newcommand{\todo}[1]{\commentatside{#1}} %compatibality with packages todo, easy-todo, todonotes
\newcommand{\TODO}[1]{\commentatside{#1}} %compatiblity with package fixmetodonotes

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
\newcommand{\eg}{e.\,g.\xspace}
\newcommand{\ie}{i.\,e.\xspace}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END COPYING HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Two kinds of cite, for the long version and the short version
\begin{document}
\title{Understanding Database Reconstruction Attacks on Public Data}
\titlerunning{Database Reconstruction}
\author{Simson Garfinkel \and John Abowd \and Christian Martindale }
\institute{U.S. Census Bureau}

\maketitle
\begin{abstract}
Statistical agencies are mandated to publish summary statistics and
micro-data while not providing data users with the ability to derive
specific attributes for particular persons or
establishments. 
Traditionally, these privacy guarantees are assured through the
use of \emph{Statistical Disclosure Limitation} (SDL)
techniques. These techniques are not sufficient to
prevent a database reconstruction attack of the sort anticipated by
Dinur and Nissim (2003). To illustrate the problem, this paper
presents a database reconstruction attack on a hypothetical block for
which statistics have been published by an official statistics
agency. We then show how the use of formally private noise reduces the
privacy risk of the database reconstruction. The paper concludes with
a discussion of the implications for the 2020 Census of Population and
Housing. 
\end{abstract}

\begin{keywords}
database reconstruction attack, SAT solver, privacy, disclosure avoidance
\end{keywords}

\section{Introduction}
In 2020 the Census Bureau will conduct the constitutionally mandated
decennial Census of Population and Housing, with the goal of counting
every person once, and only once, and in the correct place, and to
fulfill the Constitutional requirement to apportion the seats in the
U.S. House of Representatives among the states according to their
respective numbers.

Beyond the Constitutionally mandated role of the decennial census, the
US Congress has mandated many other purposes for the data. For
example, the U.S. Department of Justice uses block-by-block counts by
race for enforcing the Voting Rights Act. More generally, the results
of the decennial census, combined with other data, are used to help
distribute more than \$675 billion in federal funds to states and
local organizations.

In addition to collecting and distributing data on the American
people, the Census Bureau is charged with protecting the Privacy confidentiality of
survey responses. Specifically, all Census publications must uphold the
confidentiality standard specified by Title 13 of the U.S. Code, which
states, in part, that Bureau publications are prohibited from
identifying ``the data furnished by any particular
establishment or individual.''[Title 13, Section 9] This section
prohibits the Bureau from publishing respondent names, addresses, or any other
information that might identify a specific person or establishment.

Upholding this confidentiality requirement frequently poses a
challenge, because many statistics can inadvertently provide
information in a way that can be attributed to a particular
entity. For example, a survey of salaries in a region might report the
average salary earned by people of different occupations. However, if
there is only one bricklayer, than reporting the average salary of a
bricklayer will allow anyone who sees to statistical product to infer
that person's salary. If there are two bricklayers in the region,
reporting the average salary will allow each bricklayer to infer the
other's salary. Both of these cases would be clear violations of Title
13. The Bureau has traditionally used cell suppression to protect
privacy in situations such as this: the cells of statistical tables
that result from \emph{small counts} are suppressed and not
reported. If totals are reported, cell suppression requires that
additional \emph{complementary} cells be suppressed so that the values
of the suppressed cells cannot be worked out with simple arithmetic. 

In 2003, Dinur and Nissim showed that simple cell
suppression is not sufficient to protect the underlying confidential
data collected and used by a statistical agency to produce official
statistics\cite{DinurNissim2003}. To the contrary, they showed that
once an agency publishes more than a critical number of statistics,
the underlying confidential data can be \emph{reconstructed} by simply
finding a consistent set of microdata that, when tabulated, produce
the official statistics. One of the big surprises of the 2003 paper is
that the number of tabulations required to enable a database reconstruction
attack (DRA) is far fewer than might be intuitively expected: this is
because of the internal constraints and consistency requirements
in the published data. 

While it is mathematically impossible to prevent a reconstruction of the
underlying data, a data publisher can add noise to the published
results so that the reconstructed data will not unambiguously reveal the actual
confidential responses used to create the published
tables. Clearly, as more noise that is added, the respondents will
enjoy greater privacy protection, but the data publications will have
lower accuracy. 

So how much noise needs to be added to protect privacy? Three years
later, Dwork, McSherry, Nissim and Smith answered that question. In
their paper ``Calibrating Noise to Sensitivity in Private Data
Analysis,''\cite{Dwork:2006:CNS:2180286.2180305} the researchers
introduced the concept of \emph{differential privacy}. The paper
provides a mathematical definition of the privacy loss that
persons suffer as a result of a data publication, and proposes a
mechanism for determining how much noise needs to be added for any
given level of privacy protection.

The 2020 census is expected to count roughly 320 million people living
on roughly 8.5 million inhabited blocks, with some blocks having as
few as a single person and other blocks having thousands. With this
level of scale and diversity, it is difficult to visualize how such a
data release might be susceptible to a database reconstruction
attack. Nevertheless, with recent improvements in both computing power
and big data applications, such reconstructions now pose a significant
risk to the confidentiality of microdata that underlies unprotected
statistical tables.

To help understand the importance of adopting formal privacy methods, in
this article we presents a database reconstruction attack on a much
smaller statistical publication: a hypothetical block containing seven
people distributed over two households. We show that even a relatively
small number of constraints results in an exact solution the blocks'
inhabitants. Finally, we show how differential privacy can protect the
published data by creating uncertainty. Finally, we discuss
implications for the decennial census.

\section{An Example Database Reconstruction Attack}

To present the attack, we consider the 
census of a fictional geographic frame (for example, a suburban block),
conducted by a fictional statistical
agency. For every block, the agency collects each resident's age,
sex and race, and publishes a variety of statistics. To simplify the example,
the fictional world has only two races, black and white, and two
sexes, female and male. The statistical agency
is prohibited from publishing the raw microdata, and instead publishes
a tabular report (Table~\ref{fictional}). 

Notice that a substantial amount of information in
Table~\ref{fictional} has been suppressed (censored). In this case,
the statistical agency's disclosure avoidance rules prohibit it from
publishing statistics based on one or two people. This suppression rule is
sometimes called ``the rule of three,'' because cells in the report
sourced from fewer than three people are suppressed.

\newcommand{\cens}{\multicolumn{1}{c|}{\rule{6mm}{3mm}}}
\begin{table}
\caption{Fictional statistical data for a fictional block published by
  a fictional statistics agency. Item numbers are for identification
  purpose only.\label{fictional}}
\begin{center}
\begin{tabular}{l|l|c|c|c|}
          &                           &       & \multicolumn{2}{|c|}{Age} \\
Statistic & Group                     & Count & Median & Mean \\
\hline
       1A & total population          & 7     &  30    & 38 \\
\hline
       2A & female                    & 4     &  30    & 33.5 \\
       2B & male                      & 3     &  30    & 44 \\
       2C & Black or African American & 4     &  51    & 48.5 \\
       2D & White                     & 3     &  24    & 24 \\
\hline
       3A & single adults             & \cens & \cens  & \cens \\
       3B & married adults            & 4     & 51     & 54 \\
\hline
       4A & Black or African American female              & 3     & 36     & 36.7 \\
       4B & Black or African American male                & \cens & \cens  & \cens \\
       4C & White male                & \cens & \cens  & \cens \\
       4D & White female              & \cens & \cens  & \cens \\
\hline
       5A & persons under 5 years     & \cens & \cens  & \cens \\
       5B & persons under 18 years    & \cens & \cens  & \cens \\
       5C & persons 64 years or over  & \cens & \cens  & \cens \\
\hline
\multicolumn{5}{l}{Note: Married persons must be 15 or over}
\end{tabular}
\end{center}
\end{table}

\subsection{Encoding the Constraints}

To perform the database reconstruction attack, we view the attributes
of the persons living on the block as a collection of 
free variables. We then extract from the published table a set of
constraints. The database reconstruct attack merely finds a set of
attributes that are consistent with the constraints. If statistics are
highly constrained, the only a single reconstruction will be
possible, and that reconstruction should be the same as the underlying
microdata used to create original table.

For example, statistic 2B states that there are 3 males living in the
geography.  Because age is reported to the nearest year, and
age must necessarily be $[ 0 .. 115]$, there
are only a finite number of possible age combinations, specifically:

\[ \binom{116}{3}=\frac{116 \times 115 \times 114}{3 \times 2 \times
  1} = 253,460 \]

\input{medians} % brings in \mycount also

However, within the 253,460 possible age combinations, there are \mycount{} combinations that satisfy the
constraint of having a median of \mymedian{} and a mean of \mymean{}
(see Table~\ref{fictional}). So by applying the constraints imposed by the
published statistical tables, we are able to reduce the possible
combinations of ages for the three males from 253,460 to \mycount.

To mount a full reconstruction attack, an attacker extracts all of these
constraints and then creates a
single mathematical model that reflects them all. An automated solver can then
find an assignment of the variables that satisfies these constraints. 

To continue with our example, statistic 1A establishes the universe of
the constraint system. Because the block contains 7 people, and there
are four attributes for each (age, sex, race and marital status), we
create 28 free variables representing those four attributes for each
person. These variables are $\textrm{A}1..\textrm{A}7$ (age),
$\textrm{S}1..\textrm{S}7$ (sex), $\textrm{R}1..\textrm{R}7$ (race),
and $\textrm{M}1..\textrm{M}7$ (marital status), as shown in
Table~\ref{variables}.


\begin{table}
\caption{The variables associated with the database reconstruction
  attack. The coding for the categorical attributes is presented in the key.}\label{variables}
\begin{center}
\begin{tabular}{l|cccc}
       &     &     &      & Marital  \\
Person & Age & Sex & Race & Status   \\
\hline                             
1      & A1  & S1  & R1   & M1       \\
2      & A2  & S2  & R2   & M2       \\
3      & A3  & S3  & R3   & M3       \\
4      & A4  & S4  & R4   & M4       \\
5      & A5  & S5  & R5   & M5       \\
6      & A6  & S6  & R6   & M6       \\
7      & A7  & S7  & R7   & M7       \\
\hline
\multicolumn{1}{l}{}\\
\multicolumn{1}{l}{Key:}\\
\hline
female &     &  0  & \\
male   &     &  1  & \\
\hline
black  &     &     &  0   & \\
white  &     &     &  1   & \\
\hline
single &     &     &      &   0\\
married&     &     &      &   1\\
\hline
\end{tabular}
\end{center}
\end{table}

Because the mean age is 38, we know that:

\begin{equation}
A1 + A2 + A3 + A4 + A5 + A6 + A7 = 7 \times 38
\label{mean_age_38}
\end{equation}

We use a language called Sugar\cite{sugar} to encode the constraints
into a form that can be processed by our solver. Sugar represents
constraints as
\textit{s-expressions}\cite{McCarthy:1960:RFS:367177.367199}. For
example, equation~\ref{mean_age_38} is can be represented this way:

\begin{Verbatim}
; First define the integer variables, with the range 0..115
(int A1 0 115)
(int A2 0 115)
(int A3 0 115)
(int A4 0 115)
(int A5 0 115)
(int A6 0 115)
(int A7 0 115)

; Statistic 1A: Mean age is 30
(= (+ A1 A2 A3 A4 A5 A6 A7)
   (* 7 38)
)
\end{Verbatim}
Once the constraints in the statistical table are turned into
s-expressions, Sugar encodes the s-expressions into a
set of Boolean constraints that can be fed into a SAT-solver. For example, each age variable
is encoded using uniary notation as 116 Boolean variables. Using this
notation, the decimal value 0 is encoded as 116 false Boolean
variables, the decimal value 1 is encoded as 1 true and 115 false
values, and so on. Although this
conversion is not space efficient, it is fast, and the uniary notation makes
it easy to encode integer inequalities as simple functions of Boolean
variables. 

Although Sugar makes it relatively simple to encode constraints that
represent counts and means, encoding medians is more complicated. The
approach we settled upon requires that we first order the ages of the
person records. An unanticipated advantage of ordering the ages is
that it eliminates many degenerate solutions that different only in
permuted labels for the reconstructed individuals:

\begin{Verbatim}
;; Assure that the output is sorted by age. This does a good job 
;; eliminating dupliate answers that simply have swapped records.
;; This is called "breaking symmetry" in the formal methods literature. 
(<= A1 A2)
(<= A2 A3)
(<= A3 A4)
(<= A4 A5)
(<= A6 A7)
\end{Verbatim}

Having sorted the ages, we know that the middle variable must
be median:

\begin{Verbatim}
(= A4 30)
\end{Verbatim}

Sugar has an \texttt{if} function that allows us to encode constraints
for a subset of the population. Recall that statistic 2B contains three
constraints: there are three males, their median age is 30, and their
average age is 44. We can use the value \texttt{0} to represent a female
and \texttt{1} to represent a male:

\begin{Verbatim}
(int FEMALE 0)
(int MALE   1)
\end{Verbatim} 

Using the variable \texttt{S\emph{n}} to represent the sex of person
$n$, we then have the constraint:

\begin{equation}
S1 + S2 + S3 + S4 + S5 + S6 + S7 = 3
\end{equation}

This can be represented as:

\begin{Verbatim}
(= (+ S1 S2 S3 S4 S5 S6 S7) 3)
\end{Verbatim}

Now, using the \texttt{if} function, it is straightforward to create a constraint for
mean age of male persons is 44:

\begin{Verbatim}
(= (+ (if (= S1 MALE) A1 0)    ; average male age = 44
      (if (= S2 MALE) A2 0)
      (if (= S3 MALE) A3 0)
      (if (= S4 MALE) A4 0)
      (if (= S5 MALE) A5 0)
      (if (= S6 MALE) A6 0)
      (if (= S7 MALE) A7 0)
      )
   (* 3 44))
\end{Verbatim}

We translated Table~\ref{fictional} into \NumSExpressions{} individual
S-expressions extending over \NumConstraintLines{} lines. Sugar then
translated this into \NumVariables Boolean variables consisting of
\NumClauses clauses in the conjunctive normal form (CNF). 

Translating the constraints into CNF allows them to be solved using
either a SAT (satisfiability), SMT (satisfiability module theories),
or MIPM (mixed integer programming model) solver. There are many such
solvers, and many take input in the so-called DIMACS file
format, which is a standardized form for representing CNF equations. 
The DIMACS format was popularized by a series of annual SAT solver
competitions; one of the results of these competitions was a
tremendous speed-up of SAT solvers over the past two decades. Many
solvers can now solve CNF systems with millions of variables and
clauses in just a few minutes.

In our case, the open source
PicoSAT\cite{Biere_picosatessentials} SAT solver is able to find a
solution to the CNF problem in less than 2 seconds on a 2013 MacBook
Pro with a 2.8GHz Intel i7 processor and 16GiB of RAM (although the
program is not limited by RAM), while the open source Glucose SAT solver
can solve the problem in under 0.1 seconds on the same computer. The
stark difference between the two solvers shows the speedup possible
with an improved solving algorithm. 

\subsection{Exploring the Solution Universe}

Both solvers create a satisfying assignment for the \NumVariables{}
Boolean variables. After the solver runs, we can use Sugar to translate
these assignments back into integer values of the constructed
variables. (SMT and MIPM solvers can represent the constraints at a higher
level of abstraction, but for our purposes a SAT solver is
sufficient.)

There exists a \textit{solution universe} of all the possible
solutions to this set of constraints. If the solution
universe contains a single possible solution, then the published
statistics completely reveal the underlying confidential data. If
there are multiple satisfying solutions, then any element (person) in
common between all of the solutions is revealed. If the equations have
no solution, the set of published statistics are inconsistent.

Normally SAT, SMT and MIPM solvers will stop when they find a single
satisfying solution. One of the advantages of PicoSAT is that it can
produce the solution universe of all possible solutions to the CNF
problem. However, in this case, there is a single satisfying
assignment that produce the statistics in
Table~\ref{fictional}. That assignment is:

\begin{center}
\begin{minipage}{1.5in}
  \input{id_table_solved.tex}
\end{minipage}
\begin{minipage}{.5in}
  =
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{c}
  Solution \#1\\
  \hline
\texttt{ 8FBS}\\
\texttt{18MWS}\\
\texttt{24FWS}\\
\texttt{30MWM}\\
\texttt{36FBM}\\
\texttt{66FBM}\\
\texttt{84MBM}\\
\end{tabular}
\end{minipage}
\end{center}  
Table~\ref{fictional} is actually over-constrained: some of the
constraints can be dropped while preserving a unique
solution. For example, dropping statistic 2A, 2B, 2C or 2D still yields a
single solution, but dropping 2A \emph{and} 2B increases the solution universe to
eight satisfying solutions. All of these solutions contain the
reconstructed microdata records 8FBS, 36FBM, 66FBM and 84MBM. This
means that even if statistics 2A and 2B are censored, we can still
infer that these four microdata must be present.

Statistical agencies have long used suppression (censoring) in an
attempt to provide privacy to those whose attributes are presented in
the microdata, although the statistics that they typically drop are
those that are based on a small number of persons. How effective is
this approach?

Reviewing Table~\ref{fictional}, statistic 4A is an obvious candidate
for suppression---especially given that statistics 4B, 4C and 4D have
already been suppressed to avoid an inappropriate statistical
disclosure.

Removing the constraints for statistic 4A increases the number of
solutions from 1 to 2:

\begin{center}
\begin{tabular}{c|c}
\multicolumn{2}{c}{Solutions without 4A}\\
  Solution \#1 & Solution \#2 \\
\hline
 \texttt{ 8FBS } &  \texttt{ 2FBS} \\
 \texttt{18MWS } &  \texttt{12MWS} \\
 \texttt{24FWS } &  \texttt{24FWM} \\
 \texttt{30MWM } &  \texttt{30MBM} \\
 \texttt{36FBM } &  \texttt{36FWS} \\
 \texttt{66FBM } &  \texttt{72FBM} \\
 \texttt{84MBM } &  \texttt{90MBM} \\
  \end{tabular}
\end{center}


\section{Defending Against a DRA}\label{solution}
There are three approaches for defending against a database reconstruction: publish less
statistical data, and apply noise (random changes) to the
statistical data being tabulated, or apply noise to the results after
the tabulation. We consider them in order below.

Although it might seem that publishing less statistical data is a
reasonable defense against the DRA, this choice may severely limit the number
of tabulations that can be published. A
related problem is that, with even a moderately large population, it may be computationally infeasible to
determine when the intersection of all possible reconstructions
identifies individuals.

A second approach is to apply noise to the data before
tabulation. This approach is called \emph{input noise infusion}. For example, each respondent's age might be randomly
altered by a small amount. Input noise infusion
doesn't prevent database reconstruction, but it limits the value of
the reconstructed data by creating uncertainty for each of the
reconstructed values. 

For example, if a random offset in the range of $-2 \ldots +2$ is
added to each record of our census and the reconstruction results in individuals of
ages (7, 17, 22, 29, 36, 66, 82) or (6, 18, 26, 31, 34, 68,
82). An attacker would presumably take this into account, but they
would have no way of knowing if the true age of the youngest person is
6, 7, 8, 9 or 10. Randomness could also be applied to the sex, race
and marital status variables. Clearly, the more noise that is added,
the better privacy is protected, but the less accurate are the
resulting statistics. Considering statistic 1A, input noise infusion
might result with a median $28\ldots32$ and a mean $36\ldots40$. 

The third approach is to add noise to the statistics themselves. This
is called \emph{output noise infusion}. Whereas input noise infusion
applies noise to the microdata directly, output noise infusion applies
output to the statistical publications.  Output noise infusion impacts
database reconstruction in two ways. First, the resulting statistical
publication is likely no longer consistent, so the reconstruction of
\emph{any} database may no longer be possible. Second, even if a
database is reconstructed, it is likely not the correct database.

\section{Related Work}

In 2003 Irit Dinur and Kobbi Nissim\cite{DinurNissim2003} showed that the underlying
confidential data of a statistical database can be reconstructed with
a surprisingly small number of queries. In practice, most statistical
agencies perform these queries themselves when they release
statistical tables. Thus, Dinur and Nissim's primary finding
is that a statistical agency that publishes too many accurate statistical
tables drawn from the same confidential dataset risks inadvertently
compromising that dataset unless it takes specific protective
measures. This paper is a demonstration of that finding.

Statistical tables create the possibility of database reconstruction
because they form a set of constraints for which there is ultimately
only one exact solution. Restricting the number or specific types of
queries---for example, by suppressing results from a small number of
respondents---is often insufficient to prevent access to indirectly
identifying information, because the system's refusal to answer a
``dangerous'' query itself provides the attacker with information.
Dinur and Nissim found that, if a database is modeled as a string of
$n$ bits, then at least $\sqrt{n}$ bits must be modified by adding
noise to protect persons from being identified. In our example, each
record is contains 11 bits of data, so the confidential database has
77 bits of information. Each statistic in Table~\ref{variables}
can be modeled as a 4 bit of count, a 7 bit of median, and a 7 bit of
mean, for a total of 18 bits; Table~\ref{variables} releases 126 bits
of information.

Kasiviswanathan, Rudelson, and Smith\cite{Kasiviswanathan:2013:PLR:2627817.2627919} introduced
the concept of the linear reconstruction attack, which underlies the  DRA. The key concept is that,
given nonsensitive data such as zip code or gender, an attacker
can construct a matrix of linear equalities that can often be solved
in polynomial time. The paper also analyzes a common reconstruction
technique known as least squares decoding, where the attacker sets up
a goal function to minimize the square of the distance between two
databases in order to reconstruct the original database. This paper
does not use that attack technique, but instead creates a system of
constraints and solves them with a solver that can solve NP-hard
problems. Although such a solver requires exponential time in the
worst case, in this case it is quite fast.

\section{Conclusion}

With the dramatic improvement in both computer speeds and the
efficiency of solvers in the
last decade, database reconstruction attacks on statistical databases are no longer a 
theoretical danger. The vast quantity of data products published by
statistical agencies each year may give a determined attacker 
more than enough constraints to reconstruct some or all of a target database and
breach the privacy of millions of people. Although a suite of traditional
disclosure avoidance technique is often sufficient to defend against a
cursory attack, cell suppression and generalization are not secure
against this kind of 
attack. To protect the privacy of respondent data, statistical
agencies must use some kind of noise infusion. 
Formal privacy methods, especially differential privacy, were specifically
designed to to control this risk and, as both of our noise-infusion examples
illustrate, they do so by systematically expanding the universe of solutions
to the DBA constraints. In this expanded universe, the real confidential data
are but a single solution, and no evidence in the published data can improve 
an attacker's guess about which solution is the correct one.

\subsection{Acknowldgements}
Robert Ashmead, Chris Clifton, and Philip Leclerc provided useful
comments on this paper. Naoyuki Tamura provided invaluable help
regarding the use of Sugar.

\section{References}

\bibliographystyle{splncs03}
\bibliography{white_paper}

\section{Appendices}

\subsection{SAT and SAT Solvers}

The Boolean satisfiability problem (SAT) was the first
problem to be proven NP-complete\cite{cooklevin}. This problem asks,
for a given Boolean formula, whether replacing each variable with
either True or False can make the formula evaluate to True.  A
consequence of SAT being NP-complete is that many problems involving
constraints on variables can be
reduced in polynomial time (i.e., quickly) to an instance of the SAT
problem. Once reduced, many such problems can be solved through the
use of general-purpose heuristics. Although the SAT problem is not solvable by
algorithms in polynomial time, researchers have found many heuristic
techniques for expediting this process. SAT solvers combine a variety
of these techniques into one complex process, resulting in polynomial
time solutions for the SAT problem in many cases.

Modern SAT solvers use a heuristic technique called Conflict-Driven
Clause Learning, commonly referred to as CDCL\cite{cdcl}. Briefly, CDCL algorithm
is:

\begin{enumerate}

\item Assign a value to a variable arbitrarily.
\item Use this assignment to determine values for the other variables
  in the formula (a process known as unit propagation).
\item If a conflict is found, backtrack to the clause that made the
  conflict occur and undo variable assignments made after that point.
\item Add the negation of the conflict-causing clause as a new clause
  to the master formula and resume from step 1.

\end{enumerate}

This process is fast at solving SAT problems because adding conflicts as new clauses
has the potential to avoid wasteful ``repeated backtracks.''
Additionally, CDCL and its predecessor algorithm, DPLL, are both
provably complete algorithms and will always return either a solution
or ``Unsatisfiable'' if given enough time and memory. Another
advantage is that CDCL solvers reuse past work when producing the universe of all
possible solutions.

There are a wide variety of SAT solvers available to the public for
minimal or no cost. Although a SAT solver requires the user to
translate the problem into Boolean formulae before use, programs such
as Naoyuki Tamura's Sugar facilitate this process by translating
user-input mathematical and English constraints into Boolean formulae
automatically.

\subsection{Sugar Input}

Sugar input is given in a standard Constraint Satisfaction Problem
(CSP) file format. A constraint must be given on a single line of the
file, but here we separate most constraints into multiple lines for
readability. Constraint equations are separated by comments describing
what statistics they encode.

Input for the model in this paper is as follows:

\VerbatimInput{one-block.csp}

\end{document}

% LocalWords:  microdata equalities satisfiability uniary
