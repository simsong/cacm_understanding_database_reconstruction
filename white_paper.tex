\documentclass[runningheads]{llncs}
\include{vars}
%\newif\ifanonymized
%\anonymizedtrue
%\newif\ifshortversion
%\shortversionfalse
%\newif\iflongversion
%\longversiontrue

%\anonymizedfalse
%\shortversionfalse
%\longversiontrue
% cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
%\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[ngerman,english]{babel}
\usepackage{fancyvrb}
\usepackage{cite}
\usepackage{paralist}  % extended enumerate, such as \begin{compactenum}
%\usepackage{csquotes}  % for easy quotations: \enquote{text}
%\usepackage{microtype} % enable margin kerning
\usepackage{url}       % provides \url{...}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\usepackage{xcolor}      % required for pdfcomment later
\usepackage{pdfcomment}  % enable nice comments, also loads hyperref
\hypersetup{hidelinks,   % enable hyperref without colors and without bookmarks
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
\usepackage[all]{hypcap} %enables correct jumping to figures when referencing

\newcommand{\commentontext}[2]{%
  \colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{%
  \pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

\newcommand{\todo}[1]{\commentatside{#1}} %compatibality with packages todo, easy-todo, todonotes
\newcommand{\TODO}[1]{\commentatside{#1}} %compatiblity with package fixmetodonotes

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
\newcommand{\eg}{e.\,g.\xspace}
\newcommand{\ie}{i.\,e.\xspace}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% END COPYING HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Two kinds of cite, for the long version and the short version
\begin{document}
\title{Understanding Database Reconstruction Attacks on Public Data}
\titlerunning{Database Reconstruction}
\author{Simson Garfinkel \and John Abowd \and Christian Martindale }
\institute{U.S. Census Bureau}

\maketitle
\begin{abstract}
In 2020 the Census Bureau will conduct the constitutionally mandated
decennial Census of Population and Housing. Because a census involves
collecting large amounts of private data under the promise of
confidentiality, traditionally statistics are only published at high
levels of aggregation.  Published statistical tables are vulnerable
to a kind of \emph{database reconstruction attack}, in which the
underlying microdata are recovered merely by finding a set of
microdata that are consistent with the published statistical
tabulations. This paper shows how a database reconstruction attack can
be performed by using the tables to create a set of mathematical
constraints and then solving the resulting set of simultaneous
equations.  We then show how the attack addressed by adding noise to
the published tabulations, so that the reconstruction no longer results
in the original data. The paper concludes with a discussion of the
implications for the 2020 Census of Population and Housing.
\end{abstract}

\begin{keywords}
database reconstruction attack, SAT-solver, privacy, disclosure avoidance
\end{keywords}

\section{Introduction}
In 2020 the Census Bureau will conduct the constitutionally mandated
decennial Census of Population and Housing. The goal of the census is
to count every person once, and only once, and in the correct
place. The results will be used to fulfill the Constitutional
requirement to apportion the seats in the U.S. House of
Representatives among the states according to their respective
numbers.

Beyond the Constitutionally mandated role of the decennial census, the
US Congress has mandated many other purposes for the data. For
example, the U.S. Department of Justice uses block-by-block counts by
race for enforcing the Voting Rights Act. More generally, the results
of the decennial census, combined with other data, are used to help
distribute more than \$675 billion in federal funds to states and
local organizations.

In addition to collecting and distributing data on the American
people, the Census Bureau is charged with protecting the privacy and confidentiality of
survey responses. Specifically, all Census publications must uphold the
confidentiality standard specified by Title 13 of the U.S. Code, which
states, in part, that Bureau publications are prohibited from
identifying ``the data furnished by any particular
establishment or individual.''[Title 13, Section 9] This section
prohibits the Bureau from publishing respondent names, addresses, or any other
information that might identify a specific person or establishment.

Upholding this confidentiality requirement frequently poses a
challenge, because many statistics can inadvertently provide
information in a way that can be attributed to a particular
entity. For example, if a statistical agency accurately reported that
there were 2 persons living on a block and that the average age of the
block's residents was 35, that would constitute an improper disclosure
of personal information, because one of the residents could look up
the data, subtract their contribution, and infer the age of the other
resident. To avoid such privacy compromises, statistical agencies have
traditionally suppressed statistics resulting from a small number of
people, a process called \emph{cell suppression}, and instead
published statistics at higher levels of aggregation. Other approaches
that statistical agencies use to protect privacy are
\emph{top-coding}, in which ages higher than a certain limit are coded
as that limit, \emph{noise-infusion}, in which random values are added
to to some attributes, and \emph{swapping}, in which some of the
attributes of records representing different individuals or families
are swapped. Together, these techniques are called \emph{statistical
  disclosure limitation} (SDL).

In 2003, Dinur and Nissim showed that 
once an agency publishes more than a critical number of statistics,
the underlying confidential data can be \emph{reconstructed} by simply
finding a consistent set of microdata that, when tabulated, produce
the official statistics. This happens even if the agency only
publishes aggregate statistics. One of the big surprises of the 2003 paper is
that the number of tabulations required to enable a database reconstruction
are far fewer than might be intuitively expected: this is
because of the internal constraints and consistency requirements
in the published data. Follow-up work showed that these results could
be generalized to many more settings, and that many traditional
SDL techniques are insufficient to protect against reconstruction.

It is mathematically impossible to prevent a reconstruction of
microdata that will produce a published set of statistics. But a
statistical agency can add sufficient noise to the published tables, such that
reconstructed microdata will not match the microdata that were
collected and tabulated. 

Adding noise alone is not sufficient, however. Noise can't be added in an \emph{ad
  hoc} manner: the noise must be carefully crafted so that it disables
\emph{every possible} database reconstruction technique. 

Subsequent
publications~\cite{privacy-preserving-datamining-on-vertically-partitioned-databases,BDMN05}
refined the idea of adding noise to published tables to protect the
privacy of the individuals in the dataset. Then in 2006, Dwork, McSherry, Nissim and Smith proposed a formal framework
to for understanding these results. Their paper ``Calibrating Noise to Sensitivity in Private Data
Analysis,''\cite{Dwork:2006:CNS:2180286.2180305} 
introduced the concept of \emph{differential privacy}. The paper
provides a mathematical definition of the privacy loss that
persons suffer as a result of a data publication, and proposes a
mechanism for determining how much noise needs to be added for any
given level of privacy protection. In 2016, the paper's authors were awarded the
prestigious ``Test of Time'' award at the Theory of Cryptography
Conference (TCC)

The 2020 census is expected to count roughly 320 million people living
on roughly 8.5 million inhabited blocks, with some blocks having as
few as a single person and other blocks having thousands. With this
level of scale and diversity, it is difficult to visualize how such a
data release might be susceptible to database reconstruction. But we
now know that reconstruction would in fact pose a significant threat
to the confidentiality of the 2020 microdata that underlies
unprotected statistical tables if privacy-protecting measured are not
implemented. 

To help understand the importance of adopting formal privacy methods, in
this article we presents a database reconstruction of a much
smaller statistical publication: a hypothetical block containing seven
people distributed over two households.\footnote{The 2010 U.S. Census contained XXXX census blocks with seven or fewer people. The data can be downloaded from \url{https://www2.census.gov/census_2010/01-Redistricting_File--PL_94-171}} We show that even a relatively
small number of constraints results in an exact solution for the blocks'
inhabitants. Finally, we show how differential privacy can protect the
published data by creating uncertainty. Although readers may think
that the reconstruction of a block with just seven people is an
insignificant risk for the country as a whole, this attack can be
performed for virtually every block in the United States using the
data provided in the 2010 census. As a result, our final section discusses
implications for the 2020 decennial census.

\section{An Example Database Reconstruction Attack}

To present the attack, we consider the 
census of a fictional geographic frame (for example, a suburban block),
conducted by a fictional statistical
agency. For every block, the agency collects each resident's age,
sex and race, and publishes a variety of statistics. To simplify the example,
the fictional world has only two races, black and white, and two
sexes, female and male. The statistical agency
is prohibited from publishing the raw microdata, and instead publishes
a tabular report (Table~\ref{fictional}). 

Notice that a substantial amount of information in
Table~\ref{fictional} has been suppressed (censored). In this case,
the statistical agency's disclosure avoidance rules prohibit it from
publishing statistics based on one or two people. This suppression rule is
sometimes called ``the rule of three,'' because cells in the report
sourced from fewer than three people are suppressed.

\newcommand{\cens}{\multicolumn{1}{c|}{\rule{6mm}{3mm}}}
\begin{table}
\caption{Fictional statistical data for a fictional block published by
  a fictional statistics agency. The ``statistic'' column is for identification
  purposes only.\label{fictional}}
\begin{center}
\begin{tabular}{l|l|c|c|c|}
          &                           &       & \multicolumn{2}{|c|}{Age} \\
statistic & Group                     & Count & Median & Mean \\
\hline
       1A & total population          & 7     &  30    & 38 \\
\hline
       2A & female                    & 4     &  30    & 33.5 \\
       2B & male                      & 3     &  30    & 44 \\
       2C & black or African American & 4     &  51    & 48.5 \\
       2D & White                     & 3     &  24    & 24 \\
\hline
       3A & single adults             & \cens & \cens  & \cens \\
       3B & married adults            & 4     & 51     & 54 \\
\hline
       4A & black or African American female              & 3     & 36     & 36.7 \\
       4B & black or African American male                & \cens & \cens  & \cens \\
       4C & white male                & \cens & \cens  & \cens \\
       4D & white female              & \cens & \cens  & \cens \\
\hline
       5A & persons under 5 years     & \cens & \cens  & \cens \\
       5B & persons under 18 years    & \cens & \cens  & \cens \\
       5C & persons 64 years or over  & \cens & \cens  & \cens \\
\hline
\multicolumn{5}{l}{Note: Married persons must be 15 or over}
\end{tabular}
\end{center}
\end{table}

\subsection{Encoding the Constraints}

We can reconstruct the database by treating the attributes
of the persons living on the block as a collection of 
free variables. We then extract from the published table a set of
constraints. The database reconstruction merely finds a set of
attributes that are consistent with the constraints. If statistics are
highly constraining, then there will be a single possible
reconstruction, and those reconstructed microdata will necessarily be the same as the underlying
microdata that were used to create the original statistical publication.

For example, statistic 2B states that there are 3 males living in the
geography.  This fictional statistical agency has previously published technical
specifications that its computers internally represent each person's age as an
integer. The oldest verified age of any human being was
122\cite{whitney}. If we allow for unreported supercentenarians and
consider 125 to the oldest possible age of a human being, there
are only a finite number of possible age combinations, specifically:

\[ \binom{125}{3}=\frac{125 \times 124 \times 123}{3 \times 2 \times
  1} = 317,750 \]

\input{medians} % brings in \mycount also

However, within the 317,750 possible age combinations, there are
\mycount{} combinations that satisfy the constraint of having a median
of \mymedian{} and a mean of \mymean{} (see
Table~\ref{fictional}). (Notice that the table does not depend on the
oldest possible age, so long as it is 101 or over.) So
by applying the constraints imposed by the published statistical
tables, we are able to reduce the possible combinations of ages for
the three males from 317,750 to \mycount.

To mount a full reconstruction attack, an attacker extracts all of these
constraints and then creates a
single mathematical model that reflects them all. An automated solver can then
find an assignment of the variables that satisfies these constraints. 

To continue with our example, statistic 1A establishes the universe of
the constraint system. Because the block contains 7 people, and there
are four attributes for each (age, sex, race and marital status), we
create 28 free variables representing those four attributes for each
person. These variables are $\textrm{A}1..\textrm{A}7$ (age),
$\textrm{S}1..\textrm{S}7$ (sex), $\textrm{R}1..\textrm{R}7$ (race),
and $\textrm{M}1..\textrm{M}7$ (marital status), as shown in
Table~\ref{variables}.


\begin{table}
\caption{The variables associated with the database reconstruction
  attack. The coding for the categorical attributes is presented in the key.}\label{variables}
\begin{center}
\begin{tabular}{l|cccc}
       &     &     &      & Marital  \\
Person & Age & Sex & Race & Status   \\
\hline                             
1      & A1  & S1  & R1   & M1       \\
2      & A2  & S2  & R2   & M2       \\
3      & A3  & S3  & R3   & M3       \\
4      & A4  & S4  & R4   & M4       \\
5      & A5  & S5  & R5   & M5       \\
6      & A6  & S6  & R6   & M6       \\
7      & A7  & S7  & R7   & M7       \\
\hline
\multicolumn{1}{l}{}\\
\multicolumn{1}{l}{Key:}\\
\hline
female &     &  0  & \\
male   &     &  1  & \\
\hline
black  &     &     &  0   & \\
white  &     &     &  1   & \\
\hline
single &     &     &      &   0\\
married&     &     &      &   1\\
\hline
\end{tabular}
\end{center}
\end{table}

Because the mean age is 38, we know that:

\begin{equation}
A1 + A2 + A3 + A4 + A5 + A6 + A7 = 7 \times 38
\label{mean_age_38}
\end{equation}

We use the language Sugar\cite{sugar} to encode the constraints
into a form that can be processed by our solver. Sugar represents
constraints as
\textit{s-expressions}\cite{McCarthy:1960:RFS:367177.367199}. For
example, equation~\ref{mean_age_38} is can be represented as:

\begin{Verbatim}
; First define the integer variables, with the range 0..125
(int A1 0 125)
(int A2 0 125)
(int A3 0 125)
(int A4 0 125)
(int A5 0 125)
(int A6 0 125)
(int A7 0 125)

; Statistic 1A: Mean age is 38
(= (+ A1 A2 A3 A4 A5 A6 A7)
   (* 7 38)
)
\end{Verbatim}
Once the constraints in the statistical table are turned into
s-expressions, Sugar solves them with a brute-force attack.
Essentially, sugar explores every possible combination of the variables,
until a combination is found that satisfies the constraints. It does
this using a program called a SAT (satisfiability) solver. Using a
variety of heuristics, SAT solvers are able to rapidly eliminate many
combinations of variable assignments.

Despite their heuristic complexity, SAT solvers can only process
systems of Boolean variables, so Sugar transforms the s-expressions into a
much larger set of Boolean constraints.  For example, each age variable
is encoded using unary notation as 126 Boolean variables. Using this
notation, the decimal value 0 is encoded as 126 false Boolean
variables, the decimal value 1 is encoded as 1 true and 125 false
values, and so on. Although this
conversion is not space efficient, it is relatively fast, provided that the
integers have a limited range. 

To encode the median age constraint, we obverse that the median of a group
of numbers is the precisely defined as the value of the middle
number when the numbers are arranged in sorted order (for the case in
which there are an an odd number of numbers). Until now, we
have not distinguished persons 1 through 7 in any way: the numbers
labels are purely arbitrary. To make it easier to describe the median
constraints, we can assert that the labels must be assigned in order
of age. This is done by introducing five constraints
(Figure~\ref{breaking}), which has the side-effect of eliminating
duplicate answers that simply have swapped records, an approach called
``breaking symmetry.''\cite{10.1007/978-3-319-89960-2_6}

\begin{figure}
\begin{Verbatim}
(<= A1 A2)
(<= A2 A3)
(<= A3 A4)
(<= A4 A5)
(<= A6 A7)
\end{Verbatim}
\caption{This code fragment assures that the output is sorted by age. This does a good job 
eliminating duplicate answers that simply have swapped records, an
approach called ``breaking symmetry.''\cite{10.1007/978-3-319-89960-2_6}}\label{breaking}
\end{figure}

Having asserted that the labels are in chronological order, we can
constrain the age of the person in the middle to be the median:

\begin{Verbatim}
(= A4 30)
\end{Verbatim}

Sugar has an \texttt{if} function that allows us to encode constraints
for a subset of the population. Recall that statistic 2B contains three
constraints: there are three males, their median age is 30, and their
average age is 44. We can use the value \texttt{0} to represent a female
and \texttt{1} to represent a male:

\begin{Verbatim}
#define FEMALE 0
#define MALE   1
\end{Verbatim} 

Using the variable \texttt{S\emph{n}} to represent the sex of person
$n$, we then have the constraint:

\begin{equation}
S1 + S2 + S3 + S4 + S5 + S6 + S7 = 3
\end{equation}

This can be represented as:

\begin{Verbatim}
(= (+ S1 S2 S3 S4 S5 S6 S7) 3)
\end{Verbatim}

Now, using the \texttt{if} function, it is straightforward to create a constraint for
the mean age 44 of male persons:

\begin{Verbatim}
(= (+ (if (= S1 MALE) A1 0)    ; average male age = 44
      (if (= S2 MALE) A2 0)
      (if (= S3 MALE) A3 0)
      (if (= S4 MALE) A4 0)
      (if (= S5 MALE) A5 0)
      (if (= S6 MALE) A6 0)
      (if (= S7 MALE) A7 0)
      )
   (* 3 44))
\end{Verbatim}

We translated Table~\ref{fictional} into \NumSExpressions{} individual
S-expressions extending over \NumConstraintLines{} lines. Sugar then
translated this into a single Boolean formula consisting of
\NumVariables variables arranged in 
\NumClauses clauses. This format is called
the conjunctive normal form (CNF) because it consists of many
clauses that are combined using the Boolean AND operation.

Interestingly, we can even create constraints for the suppressed data!
Statistic 3A is suppressed, so we know that there are 0, 1 or 2 single
adults. We let $\textrm{M}n$ represent the marital status of person
$n$:

\begin{Verbatim}
#define SINGLE 0
#define MARRIED 1

(int SINGLE_ADULT_COUNT 0 2)
(= (+ (if (and (= M1 SINGLE) (> A1 17)) 1 0)
      (if (and (= M2 SINGLE) (> A2 17)) 1 0)      
      (if (and (= M3 SINGLE) (> A3 17)) 1 0)      
      (if (and (= M4 SINGLE) (> A4 17)) 1 0)      
      (if (and (= M5 SINGLE) (> A5 17)) 1 0)      
      (if (and (= M6 SINGLE) (> A6 17)) 1 0)      
      (if (and (= M7 SINGLE) (> A7 17)) 1 0))
  SINGLE_ADULT_COUNT)

(>= SINGLE_ADULT_COUNT 0)
(<= SINGLE_ADULT_COUNT 2)
\end{Verbatim}


Translating the constraints into CNF allows them to be solved using
any solver that can solve an NP-complete program, such as a SAT
solver, a SMT (satisfiability module theories) solver,
or MIP (mixed integer programming) solver. There are many such
solvers, and most take input in the so-called DIMACS file
format, which is a standardized form for representing CNF equations. 
The DIMACS format was popularized by a series of annual SAT solver
competitions; one of the results of these competitions was a
tremendous speed-up of SAT solvers over the past two decades. Many
solvers can now solve CNF systems with millions of variables and
clauses in just a few minutes, although some problems do take much
longer. Marijn Heule and Oliver Kullmann discussed the rapid
advancement and use of SAT solvers in their August 2017 \emph{ACM Communications}
cover story, ``The Science of Brute Force''~\cite{Heule:2017:SBF:3127343.3107239}.

In our case, the open source
PicoSAT\cite{Biere_picosatessentials} SAT solver is able to find a
solution to the CNF problem in less than 2 seconds on a 2013 MacBook
Pro with a 2.8GHz Intel i7 processor and 16GiB of RAM (although the
program is not limited by RAM), while the open source Glucose SAT solver
can solve the problem in under 0.1 seconds on the same computer. The
stark difference between the two solvers shows the speedup possible
with an improved solving algorithm. 

\subsection{Exploring the Solution Universe}

Both solvers create a satisfying assignment for the \NumVariables{}
Boolean variables. After the solver runs, we can use Sugar to translate
these assignments back into integer values of the constructed
variables. (SMT and MIPM solvers can represent the constraints at a higher
level of abstraction, but for our purposes a SAT solver is
sufficient.)

There exists a \textit{solution universe} of all the possible
solutions to this set of constraints. If the solution
universe contains a single possible solution, then the published
statistics completely reveal the underlying confidential
data---provided that noise was not added to either the microdata or
the tabulations as a disclosure avoidance mechanism. If
there are multiple satisfying solutions, then any element (person) in
common between all of the solutions is revealed. If the equations have
no solution, the set of published statistics are inconsistent. This
doesn't mean that a high-quality reconstruction is not
possible. Instead of using the published statistics as a set of
constraints, they can be used as inputs to a multidimensional
objective function: the system can then be solved using another kind
of solver called an optimizer.

Normally SAT, SMT and MIPM solvers will stop when they find a single
satisfying solution. One of the advantages of PicoSAT is that it can
produce the solution universe of all possible solutions to the CNF
problem. However, in this case, there is a single satisfying
assignment that produce the statistics in
Table~\ref{fictional}. That assignment is:

\begin{center}
\begin{minipage}{1.5in}
  \input{id_table_solved.tex}
\end{minipage}
\begin{minipage}{.5in}
  =
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{c}
  Solution \#1\\
  \hline
\texttt{ 8FBS}\\
\texttt{18MWS}\\
\texttt{24FWS}\\
\texttt{30MWM}\\
\texttt{36FBM}\\
\texttt{66FBM}\\
\texttt{84MBM}\\
\end{tabular}
\end{minipage}
\end{center}  
Table~\ref{fictional} over-constrains the solution universe: some of the
constraints can be dropped while preserving a unique
solution. For example, dropping statistic 2A, 2B, 2C or 2D still yields a
single solution, but dropping 2A \emph{and} 2B increases the solution universe to
eight satisfying solutions. All of these solutions contain the
reconstructed microdata records 8FBS, 36FBM, 66FBM and 84MBM. This
means that even if statistics 2A and 2B are censored, we can still
infer that these four microdata must be present.

Statistical agencies have long used suppression (censoring) in an
attempt to provide privacy to those whose attributes are presented in
the microdata, although the statistics that they typically drop are
those that are based on a small number of persons. How effective is
this approach?

Reviewing Table~\ref{fictional}, statistic 4A is an obvious candidate
for suppression---especially given that statistics 4B, 4C and 4D have
already been suppressed to avoid an inappropriate statistical
disclosure.

Removing the constraints for statistic 4A increases the number of
solutions from 1 to 2:

\begin{center}
\begin{tabular}{c|c}
\multicolumn{2}{c}{Solutions without 4A}\\
  Solution \#1 & Solution \#2 \\
\hline
 \texttt{ 8FBS } &  \texttt{ 2FBS} \\
 \texttt{18MWS } &  \texttt{12MWS} \\
 \texttt{24FWS } &  \texttt{24FWM} \\
 \texttt{30MWM } &  \texttt{30MBM} \\
 \texttt{36FBM } &  \texttt{36FWS} \\
 \texttt{66FBM } &  \texttt{72FBM} \\
 \texttt{84MBM } &  \texttt{90MBM} \\
  \end{tabular}
\end{center}


\section{Defending Against a DRA}\label{solution}
There are three approaches for defending against a database
reconstruction. The first is to publish fewer statistical data---this
is the approach taken by legacy disclosure avoidance techniques (cell
suppression, top-coding and generalization). The second and third
approaches involve adding noise, or randomness. Noise can be added to
the statistical data being tabulated, or to the results after
tabulation. We consider each of these below.

\paragraph{Option 1: Publisher fewer data.} Although it might seem that publishing less statistical data is a
reasonable defense against the DRA, this choice may severely limit the
number of tabulations that can be published. A related problem is
that, with even a moderately small population, it may be
computationally infeasible to determine when the published statistics
still identify a sizable fraction of individuals in the population. 

\paragraph{Option 2: Apply noise before tabulation.} This approach is called \emph{input noise infusion}. For example, each respondent's age might be randomly
altered by a small amount. Input noise infusion doesn't prevent
finding a set of microdata that are consistent with the published
statistics, what we call \emph{database reconstruction}, but it limits
the value of the reconstructed microdata, since what is reconstructed
are the microdata \emph{after} the noise has been added.

For example, if a random offset in the range of $-2 \ldots +2$ is
added to each record of our census and the reconstruction results in individuals of
ages (7, 17, 22, 29, 36, 66, 82) or (6, 18, 26, 31, 34, 68,
82). An attacker would presumably take this into account, but they
would have no way of knowing if the true age of the youngest person is
5, 6, 7, 8 or 9. Randomness could also be applied to the sex, race
and marital status variables. Clearly, the more noise that is added,
the better privacy is protected, but the less accurate are the
resulting statistics. Considering statistic 1A, input noise infusion
might result with a median $28\ldots32$ and a mean $36\ldots40$.
(Note that when using differential privacy, the infused noise is not
drawn from a bounded domain, but instead is typically drawn from a
Laplace or geometric distribution.)

Swapping, the disclosure avoidance approach used in the 2010 Census,
is a kind of input noise infusion. In swapping, some of the attributes
are exchanged, or \emph{swapped}, between records. The advantage of
swapping is that it has no impact on some kinds of statistics: if
people are only swapped within a county, than any tabulation the
county level will be unaffected by swapping. The disadvantage of
swapping is that it can have significant impact on statistics at lower
levels of geography, and values that are not swapped are unprotected.

\paragraph{Option 3: Apply noise to the published statistics.} This
approach is called \emph{output noise infusion}. Whereas input noise
infusion applies noise to the microdata directly, output noise
infusion applies output to the statistical publications.  Output noise
infusion complicates database reconstruction by eliminating na\"ive
approaches based on the straightforward application of SAT
solvers. Second, even if a set of microdata are constructed that are
mostly consistent with the published statistics, those microdata will
be somewhat different from the original microdata that were
collected. The more noise that was added to the tabulation, the more
different the microdata will be.

When noise is added to either the input data (option 2) or
the tabulation results (option 3), with all records having equal
probability of being altered, it is possible to mathematically
describe the resulting privacy protection. This is the basis of
differential privacy.

\subsection{Implications for the 2020 Census}

The U.S. Census Bureau has announced that it is adopting a noise
infusion mechanism based on differential privacy to provide privacy
protection for the underlying microdata that are collected as part of
the 2020 census. Here we provide more of the motivation for that
decision. 

The protection mechanism developed for the 2010 Census was based on a
technique called swapping and that is described
elsewhere\cite{swapping}. The swapping technique was not designed to
protect the underlying data against a database reconstruction
attack. Indeed, it is the Census Bureau's policy that both the swapped
and the unswapped microdata are considered confidential.

The 2010 census found a total population of 308,745,538. These people
occupied in 10,620,683 habitable blocks. Each person was located in a
residential household or institutional housing arrangement (what the
Census Bureau called a ``group quarters''). For each person, the Census
bureau tabulated the person's location as well as their sex, age,
race and ethnicity, and their relationship
to the head of the household---that is, five attributes per person,
for a total of approximately 1.5 billion attributes in total. Using
this data, the Bureau published approximately 7.7 billion statistics,
including 2.7 billion in the PL94-171 redistricting file, 2.8 billion
in the balance of Summary File 1, 2 billion in summary file 2, and 31
million records in a public-use microdata sample. This is
approximately 25 statistics per person. Given these numbers and the
worked example in this article, it is clear that there is a
theoretical possibility that the national level census could be
reconstructed, although tools such as Sugar and PicoSAT are probably
not powerful enough to do so.

To protect the privacy of census respondents, the Bureau is developing
a privacy protection system based on differential privacy. This system
will assure that every statistic and the corresponding microdata
receives some amount of privacy protection, while providing that the
resulting statistics are sufficiently accurate for their intended
purpose.

In this publication we have explained the motivation for the decision
to use differential privacy. Without a privacy
protection system based on noise infusion, it would be possible to
reconstruct accurate microdata using only the published statistics. By
using differential privacy, we can add the minimum amount of noise
necessary to achieve our privacy requirements. A future article will
explain how that system works.


\section{Related Work}

In 2003 Irit Dinur and Kobbi Nissim\cite{DinurNissim2003} showed that
the amount of noise that needs to be added to a database to prevent a
reconstruction of the underlying data is on the order of $\Omaga\sqrt{n}$
where $n$ is number of bits in the database. In practice, many statistical
agencies do not add this much noise when they release
statistical tables. (In our example, each
record is contains 11 bits of data, so the confidential database has
77 bits of information. Each statistic in Table~\ref{variables}
can be modeled as a 4 bit of count, a 7 bit of median, and a 7 bit of
mean, for a total of 18 bits; Table~\ref{variables} releases 126 bits
of information.) Dinur and Nissim's primary finding
is that many statistical agencies leave themselves open to the risk of
database reconstruction. This paper demonstrates one way to
conduct that attack.

Statistical tables create the possibility of database reconstruction
because they form a set of constraints for which there is ultimately
only one exact solution. Restricting the number or specific types of
queries---for example, by suppressing results from a small number of
respondents---is often insufficient to prevent access to indirectly
identifying information, because the system's refusal to answer a
``dangerous'' query itself provides the attacker with information.


%% Kasiviswanathan, Rudelson, and
%% Smith\cite{Kasiviswanathan:2013:PLR:2627817.2627919} introduced the
%% concept of the linear reconstruction attack, which underlies the
%% DRA. The key concept is that, given nonsensitive data such as zip
%% code or gender, an attacker can construct a matrix of linear
%% equalities that can often be solved in polynomial time. The paper
%% also analyzes a common reconstruction technique known as least
%% squares decoding, where the attacker sets up a goal function to
%% minimize the square of the distance between two databases in order
%% to reconstruct the original database. This paper does not use that
%% attack technique, but instead creates a system of constraints and
%% solves them with a solver that can solve NP-hard problems. Although
%% such a solver requires exponential time in the worst case, in many
%% cases it is quite fast.

\section{Conclusion}

With the dramatic improvement in both computer speeds and the
efficiency of solvers in the last decade, database reconstruction
attacks on statistical databases are no longer a theoretical
danger. The vast quantity of data products published by statistical
agencies each year may give a determined attacker more than enough
information to reconstruct some or all of a target database and breach
the privacy of millions of people. Traditional disclosure avoidance
technique are not designed to protect against this kind of attack.

Faced with the threat of database reconstruction, statistical agencies
have two choices: they can either publish dramatically less
information, or they can use some kind of noise infusion.  Agencies
can use differential privacy to determine the minimum
amount of noise necessary to add, and most efficient way to add that
noise, in order to achieve their privacy protection goals.

\subsection{Acknowledgments}
Robert Ashmead, Chris Clifton, Kobbi Nissim, and Philip Leclerc
provided extraordinarily useful
comments on this paper. Naoyuki Tamura provided invaluable help
regarding the use of Sugar.

\section{References}

\bibliographystyle{splncs03}
\bibliography{white_paper}

\section{Appendices}

\subsection{SAT and SAT Solvers}

The Boolean satisfiability problem (SAT) was the first
problem to be proven NP-complete\cite{cooklevin}. This problem asks,
for a given Boolean formula, whether replacing each variable with
either True or False can make the formula evaluate to True.  Modern SAT
solvers work well and reasonably quickly for a large variety of SAT
problem instances, and up to reasonably large instance sizes. 

Many modern SAT solvers use a heuristic technique called Conflict-Driven
Clause Learning, commonly referred to as CDCL\cite{cdcl}. Briefly, CDCL algorithm
is:

\begin{enumerate}

\item Assign a value to a variable arbitrarily.
\item Use this assignment to determine values for the other variables
  in the formula (a process known as unit propagation).
\item If a conflict is found, backtrack to the clause that made the
  conflict occur and undo variable assignments made after that point.
\item Add the negation of the conflict-causing clause as a new clause
  to the master formula and resume from step 1.

\end{enumerate}

This process is fast at solving SAT problems because adding conflicts as new clauses
has the potential to avoid wasteful ``repeated backtracks.''
Additionally, CDCL and its predecessor algorithm, DPLL, are both
provably complete algorithms and will always return either a solution
or ``Unsatisfiable'' if given enough time and memory. Another
advantage is that CDCL solvers reuse past work when producing the universe of all
possible solutions.

There are a wide variety of SAT solvers available to the public for
minimal or no cost. Although a SAT solver requires the user to
translate the problem into Boolean formulae before use, programs such
as Naoyuki Tamura's Sugar facilitate this process by translating
user-input mathematical and English constraints into Boolean formulae
automatically.

\subsection{Sugar Input}

Sugar input is given in a standard Constraint Satisfaction Problem
(CSP) file format. A constraint must be given on a single line of the
file, but here we separate most constraints into multiple lines for
readability. Constraint equations are separated by comments describing
what statistics they encode.

Input for the model in this paper is as follows:

\VerbatimInput{one-block.csp}

\end{document}

% LocalWords:  microdata equalities satisfiability  Simson Garfinkel
% LocalWords:  Abowd Martindale SDL Dinur Nissim hoc dataset Dwork na
% LocalWords:  McSherry TCC supercentenarians cccc unary CNF SMT MIP
% LocalWords:  DIMACS Marijn Heule Kullmann ACM PicoSAT MacBook GiB
% LocalWords:  MIPM FBS MWS FWS MWM FBM MBM FWM DRA ive unswapped CSP
% LocalWords:  Irit Kobbi Nissim's Ashmead Leclerc Naoyuki Tamura csp
% LocalWords:  CDCL DPLL Unsatisfiable Tamura's
